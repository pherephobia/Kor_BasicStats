--- 
title: "Basic Statistics and Data Analysis: Korean Version"
subtitle: "기초 통계와 데이터 관리: 한글판"
author: "Sanghoon Park"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
header-includes:
  - \usepackage{kotex}
  - \usepackage{mathspec}
  - \usepackage{amsmath,amsthm}
  - \usepackage{graphicx}
  - \usepackage{setspace}\onehalfspacing
output: 
  pdf_document: 
    latex_engine: xelatex
    fig_height: 6
    fig_width: 10
    toc: no
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
mainfont: NanumGothic
fontsize: 12pt
urlcolor: blue
---

# Prerequisites

원래 저 같은 경우에는 통계분석을 할 때 주력 툴(tool)로 STATA를 써왔습니다. 하지만 한국에서 STATA로 대학원생이 연구를 수행하는 데 있어서 한 가지 걸림돌이 되는 문제는 바로 'Copyrights'에 있습니다.

STATA는 유저들 사이에서 온라인/오프라인 등을 통하여 여러가지 매뉴얼들과 피드백이 이루어져 강력한 이점을 지닌 도구인 것은 사실이지만 그 가격이 만만치가 않습니다. 적어도 제가 석사까지 마쳤던 학교에서는 STATA를 설치하는 데 있어서 학교 차원의 지원 등은 없었기 때문에 높은 가격을 감수하고 구매하던가 혹은 어떻게든(?) 구해서 사용하는 방법밖에는 없었습니다.

그에 비해서 **R**은 오픈소스라 어느 정도 자유롭게 접근할 수 있습니다. 또한 STATA 못지 않게 폭넓게 유저들 간의 소통을 통해 매뉴얼이 제공되며, 피드백이 이루어지는 강력한 통계패키지입니다.[^1-1] 

정치학을 공부하는 입장에서 STATA에서 R로 갈아타는 데 가장 어려웠던 점은 초입의 진입장벽이었다고 할 수 있습니다. 일종의 Trade-offs 관계로까지 느껴졌던 것이 **R**의 유연함은 곧 어떠한 결과를 얻기 위해서는 하나하나의 요소를 유저가 직접 조합할 수 있어야 하고, 각 요소의 특성을 파악해야 한다는 것이었습니다. 예를 들어, STATA에서는 변수의 특성 등 통계학에서 일반적으로 고려하는 부분들에 집중하면 되었지만 **R**에서는 객체(objects)의 특성들(List인지, Character인지, Factor인지, Vector인지 등)을 살펴보아야 했습니다. 만약 객체 특성을 고려하지 않을 경우에 결과가 크게 달라질 수 있기 때문입니다. 이러한 진입장벽의 문제가 종종 **R**을 시작하자마자 포기하게끔 만드는 결과로 이어지는 경우를 종종 보았습니다.

이 Git은 데이터 분석의 기본을 소개하는 데 목적을 가지고 있습니다. Git에 올라오는 자료들을 이해하는 데 있어서 수학적인 배경지식은 거의 요구되지 않지만 간단한 응용수학들에 대해서는 다루게 됩니다. 이 Git의 자료들은 통계분석을 수행하는 데 필수적인 개념적 지식들을 익숙하게 하는 것에 있습니다. 또한 **R**을 이용하여 데이터 관리 및 가시화(visualization)에 필요한 실용적인 기법들을 익히는 것을 기대합니다. 나아가 정량연구(quantitative research)에서 요구되는 재현가능성(replicability)와 투명성(transparency)을 제고할 수 있는 일련의 작업환경들을 만드는 훈련을 같이 합니다. 이 Git의 내용은 어디까지나 기본적인 내용들을 담고 있기 때문에 정량연구방법을 마스터하는 첫 걸음으로써는 적절한 내용들을 담고 있을 것이라고 기대합니다. 정리하면, 이 Git의 자료들을 통해 기대되는 성과는 다음과 같습니다.

  + 데이터에서 변수들의 관계를 서술하고 평가하는 데 필요한 통계방법을 이용하고 사용할 수 있을 것입니다.
  + 데이터를 보여주기 위해 유용한 그래픽을 활용할 수 있게 될 것입니다.
  + 데이터 접근성과 연구 투명성 원칙들을 자신의 연구에 충분히 적용할 수 있게 될 것입니다.
  + **R**을 이용하여 데이터를 관리 및 분석할 수 있게 될 것입니다.
  + *LaTex*를 이용하여 과학적 연구의 결과를 페이퍼와 발표자료 등으로 구성해낼 수 있을 것입니다.
  
이 Git의 주요 자료들은 다음과 같은 자료에 기초하여 작성되었습니다.

  + Diez, David D., Christopher D. Barr, and Mine Çetinkaya-Rundel. 2019. *OpenIntro Statistics. Fourth Edition*. 무료로 [여기](https://www.openintro.org/stat/textbook.php?stat_book=os)에 공개되어 있습니다.
  + Lander, Jared P. 2013. *R for Everyone: Advanced Analytics and Graphics*. ISBN-13: 978-0321888037

이외에도 추가로 다음과 같은 홈페이지의 코스들로부터 도움을 받으실 수 있습니다.

  + 통계분석에 필요한 수학적 지식의 기본적인 내용들을 다시 복습하시기에 좋은 자료입니다: [Havard's Math Prefresher](https://projects.iq.harvard.edu/prefresher)
  + 주제별로 **R**을 활용한 분석에 대한 다양한 자료를 제공합니다: [DataCamp](https://www.datacamp.com/courses/free-introduction-to-r)

**R**은 일단 하나의 언어라고 생각할 수 있습니다. **R** 자체로도 함수들을 이용해 우리가 원하는 분석을 할 수 있겠지만, 굉장히 편리하고 유용한 플랫폼을 통해서 보다 용이하게, 편리하게 **R**을 사용할 수 있습니다. 바로 *RStudio*입니다. *RStudio*는 Graphical User Interface(GUI)로 **R**을 좀 더 직관적으로 사용하는 데 도움을 줍니다.

  + **R**을 다운로드 하시려면 [여기](https://cloud.r-project.org/)
  + **R**을 설치하고 나서 플랫폼으로 설치할 [RStudio](https://www.rstudio.com/products/rstudio/download/#download)



[^1-1]: 피드백이 잘 이루어진다는 것은 실제 연구분석에 적용되는 패키지의 활용에 대해 커뮤니티에서의 소통이 용이하고, 분석에 필요한 새로운 패키지들에 대한 접근성이 높다는 것을 의미합니다.

<!--chapter:end:index.Rmd-->

# Introduction to **R** {#intro}

**R**은 프로그래밍 언어에 가까운데 통계분석에도 꽤나 특화되어 있습니다. 요즘 *Python* 등도 주목받고 있지만 사회과학적 정량연구에 적용하는 정도로 통계분석을 수행하는 유관 학과들에서는 **R**을 많이 사용하는 것 같습니다. **R**은 단순하지만 동시에 복합적인 조합을 통해서 여러가지 분석을 수행할 수 있기 때문입니다.[^2-1] 

다음 포스팅에서는 *RStudio*를 이용한 **R**의 기본적인 요소들을 소개할 것입니다. *RStudio*에 대한 간략한 소개는 [다음](http://sphweb.bumc.bu.edu/otlt/MPH-Modules/QuantCore/PH717-R-Basics/PH717-R-Basics3.html)의 *RStudio* 공식 링크에서 살펴볼 수 있습니다.

## 기본 동작에 관한 소개

일단 **R**에서 일반적으로 사용하는 코드를 제외하고 코멘트를 달고 싶을 때에는 `#` 를 사용합니다. `#` 기호 뒤에 쓴 글들은 **RStudio**에서 작동되지 않스니다. 코멘트를 작성하는 습관을 들여놓는 게 좋은데, 본인이 만들어놓은 코드와 데이터셋 이름을 까먹어서 고생하지 않도록 해주고 다른 사람이 코드를 읽을 때에도 꽤나 유용하기 때문입니다. 

  + 작동코드를 작성하기 전에 쓰는 설명은 보통 `##`로, 작동코드 옆에 병기하는 인라인 코드(in-line code)는 `#`로 구분하여 코멘트를 작성해줍니다.
    + 즉, `##`은 한 줄이 아예 전체 코멘트일 때, `#`은 **R** 코드 옆에 해당 코드의 내용에 대해 코멘트할 때 사용합니다.
  + 예를 들면, 아래와 같은 코드에서 첫 번째 줄은 단순히 어떠한 분석을 수행할지 알려주는 역할만 하고, 아래의 코드가 실제로 작동될 것입니다.
    ```{r}
    ## 1+1을 계산해봅시다.
    1 + 1  # 답은 2
    ```

**R**을 배우면 배울수록, 여러가지 노하우가 체화되고 보다 효율적인 코딩을 시도하게 되는데, 일단 간단한 팁들은 [여기](https://google.github.io/styleguide/Rguide.xml)에서 살펴볼 수 있습니다. 이러한 팁과 노하우들에 익숙해지면 코드를 읽고, 공유하고, 짜는 게 조금 더 쉬워질 것이라고 생각합니다.

## **R**의 연산자(operations)

일단 사회과학을 연구하고자 하는 입장에서 **R**을 사용하고자 하니, **R**이 가진 단순한 수학연산자들이 어떻게 기능하는지를 살펴볼 필요가 있습니다.

  + **R** 안에서 어떠한 수학연산자들이 있는지 살펴보고 싶다면 *RStudio* 좌측 하단 콘솔에 `?"*"`라고 입력해보면 됩니다.
  + 사실 코드블럭의 왼쪽 기호는 몰랐을 수도 있는데, 오른쪽의 의미를 모를 것이라고는 생각하지 않습니다. 따라서 자세한 설명은 넘어가고... 이 중에서 마지막 기호는 조금 익숙해질 필요는 있습니다. 저도 아직 연구에서 변수 조작(manipulation)할 때 사용해본 적은 없는 기호입니다만, 평소에는 크게 쓸 일이 없기 때문에 까먹지 않도록 하는 것이 중요할 것 같습니다.
  + 마지막 기호를 이용해서 할 수 있는 대표적인 작업이 루프에서 특정 변수가 홀수인지 짝수인지 구분하게 하는 것 등입니다. 만약 2로 나누어 나머지가 남으면 홀수, 남지 않으면 짝수라고 생각해볼 수 있습니다.
    ```{r}
    ## 제곱해보기: a^b라고 할 때, a를 b만큼 곱해주는 것
    3^2 #9
    2^3 #8
    ## 나머지 구하기 : a%%b라고 할 때, a를 b로 나누고 몫이 아닌 나머지를 보여줍니다.
    27 %% 7 #6
    ## 마지막으로 연산자를 가지고 계산할 때, 그리고 연산자 뿐 아니라 코딩 전체에 있어서
    ## 순서를 잘 고려하여 코딩해야 합니다. 아래 두 계산은 완전히 결과가 다릅니다.
    3 + 4 / 7
    (3 + 4) / 7
    ```


## **R**의 기본적인 자료 유형

**R**은 여러 가지 자료 유형을 제공합니다. 예를 들어 바로 위에서 계산할 때 사용하였던 숫자는 말 그대로 **R**에서 숫자형(numeric)으로 간주됩니다. 기본적으로 반드시 알아두어야 할 자료 유형은 다음과 같습니다.

  + 숫자형(numeric): 1.111 같이 소수점 값을 가지는 자료 유형입니다.
  + 정수형(integer): 2와 같은 자연수를 말하는 데, 소수점 값을 갖지 않는 것. 숫자형이랑 비슷합니다.
  + 논리형(logical): 부울리안 값(Boolean values), 참(TRUE)/거짓(FALSE)을 가지는 자료 유형입니다.
  + 문자형(character): 문자열(text or string)의 값을 지니는 것으로 인용자("")를 사용하여 입력합니다.

자료 유형을 살펴보았으니, 잠깐 변수 배정(variable assignment)에 대해 얘기해보겠습니다. 뭐랄까, 걍 어떤 변수를 만드는 거라고 생각하면 됩니다. 변수는 통계학에서 가장 기본적인 개념 중 하나이니 따로 설명할 필요는 없을 것 같습니다.

**R**에서 우리는 어떤 값(values)을 객체(object)에 저장합니다. 이때 객체는 함수(functions)일 수도 있고 그래프(plots)이거나 혹은 데이터셋(datasets)일 수도 있습니다. 즉, 그냥 콘솔에다가 2를 치면 결과 창에 2가 나오기는 하겠지만 그거 자체를 바로 분석에 사용할 수는 없고, 사용한다고 하더라도 지속적이지 않습니다. 계속 그 값을 써먹으려면 우리도 그 값에 이름을 붙여줘야 합니다. 그 값의 성격에 따라서 그것이 저장된 용기(container)의 이름도 바뀐다고 생각하시면 편합니다. 어떤 식 자체를 저장했다면 함수의 형태로, 혹은 숫자만 넣어놨다면 걍 숫자 하나가 담긴 객체가 될 것입니다. **R**에서 변수를 배정하기 위해서는 <- 나 =의 기호를 사용하여야 한다. 사용해본 경험으로는 <-를 더 추천합니다.[^2-2] 

```{r}
## 사과라는 변수에다가 값을 집어넣어 보겠습니다
apples <- 4
## 사과에 담긴 값을 출력합니다.
apples
```

이제는 객체/변수와 연산자를 같이 사용하여 계산을 해보겠습니다.

```{r}
## 오렌지라는 변수에다가 값을 집어넣어 보겠습니다.
oranges <- 6 
## 오렌지에는 6이 들어가 있고 사과에는 4가 들어가 있습니다. 두 개를 더해 보겠습니다.
apples + oranges
```

다음으로는 숫자가 아니라 다른 형태의 자료를 담아보겠습니다. 주의할 점은 자료 형태가 서로 다른 변수들 끼리는 연산자를 통해 계산할 수 없다는 점입니다.

```{r, error=TRUE}
## 오렌지 객체에 문자열 자료를 다시 저장해봅니다.
oranges <- "six" 
## 오렌지와 사과를 더해 보겠습니다.
apples + oranges # 에러메세지를 확인할 수 있습니다.
```

### 논리형 연산자 (Logical operators)

앞서 수학연산자를 간단하게 살펴보았는데, 이번에는 논리형 연산자를 한 번 살펴 보겠습니다. 논리형 연산자는 부울리안 값(`TRUE` or `FALSE`)을 나타냅니다. 논리형 연산자는 아래와 같고, 좀 더 구체적인 내용은 [여기](https://www.statmethods.net/management/operators.html)에서 확인할 수 있습니다.

```{r, eval = FALSE}
a < b   # a가 b보다 작다는 것을 보여줍니다.
a <= b  # a가 b보다 작거나 같다는 것을 보여줍니다.
a > b   # a가 b보다 크다는 것을 보여줍니다.
a >= b  # a가 b보다 크거나 같다는 것을 보여줍니다.
a == b  # a와 b가 같다/동일하다는 것을 보여줍니다.
!a      # a가 아니라는 의미입니다.
```

위의 연산자를 가지고 아래의 연습을 해보겠습니다.

```{r}
## 1이 2보다 작을까?
1 < 2 # TRUE, 사실이라는 결과를 얻을 것입니다.
## 1 더하기 1이 3일까?
1 + 1 == 3 # FALSE, 거짓이라는 결과를 얻을 것입니다.
```

**R**에서 `TRUE`는 1과 같고, `FALSE`는 0과 같습니다. 그렇다면 다음의 연습을 해보겠습니다.

```{r}
apples <- 4 
oranges <- TRUE 
apples + oranges
```

**R**은 매우 까다롭습니다. 하나라도 다르면 기대한대로 결과가 나오지 않거나 작동하지 않고 에러메세지를 띄우기도 다반사입니다. 아래의 경우를 살펴보겠습니다.

```{r}
## 대문자와 소문자를 가리는 R
oranges <- "six" 
Oranges <- "Six" 
oranges == Oranges # six와 Six는 앞의 문자가 하나 다르기 때문에 
                   # FALSE라는 결과를 얻을 것입니다.
```


## 벡터 (Vectors)

앞서 변수에 대해서 얘기했는데, 이번에는 벡터에 대해서 살펴볼 것입니다. 벡터는 원하는 만큼 많은 데이터를 일차원(one-dimension)에 배열할 수 있는 형태의 자료로, **R**에서 벡터를 만들기 위해서는 `c()` 형태의 함수를 이용합니다. 이 함수의 괄호 내부에 원하는 요소들을 콤마(`,`)를 이용해 배열하면 하나의 벡터에 담을 수 있습니다.

```{r}
## 숫자형 자료들이 담긴 벡터를 만들어 보겠습니다.
num_vec <- c(1, 2, 3)
```

이번에는 문자형 자료가 담긴 벡터와 논리형(부울리안) 값을 가진 벡터를 만들어 보겠습니다. 만들고 난 이후에 `class()`나 `typeof()` 함수를 이용하여 벡터에 어떠한 형태의 자료가 담겼는지를 확인할 수 있습니다.

```{r}
## 여러 자료 유형을 이용하여 벡터를 만들어 보겠습니다.
mix_vec <- c(1, "Hi", TRUE)
class(mix_vec)  # Character라는 답을 얻게 됩니다.
typeof(mix_vec)
```

하나라도 다른 유형의 자료가 벡터 안에 포함되면 기대한 결과를 얻지 못할 수 있기 때문에 항상 `class()` 함수로 확인해주는 것이 필요합니다. 그리고 벡터도 수학연산자들을 이용해 일종의 계산이 가능한데, 단, **R**은 굉장히 '까다롭다'는 것을 기억하셔야 합니다. 벡터의 경우에는 요소 하나하나를 구별해서 인식하기 때문입니다. 다음의 예를 살펴 보겠습니다.

```{r}
c(1, 2, 3) + c(4, 5, 6) 
c(1 + 4, 2 + 5, 3 + 6)
```

위의 두 결과는 동일합니다. 정확히는 위의 식을 아래와 같은 식으로 **R**이 계산하여 결과를 보여준다고 하는 것이 맞을 것입니다. 다른 연산자들은 어떨까요? 그리고 만약 벡터의 요소 개수가 서로 다르면(보통 길이가 다르다고 한다) 어떻게 될까요? 다음 코드를 통해 한 번 살펴 보겠습니다.

```{r}
c(1, 2, 3) * c(4, 5, 6)     # 4, 10, 18의 결과값을 얻게 될 것입니다
c(1, 2) + c(4, 5, 6, 7, 8)  # 5, 7, 7, 9, 9의 결과를 얻게 되고, 두 벡터의 길이가
                            # 다르다는 경고 메시지를 보게 될 것입니다.
c(1, 2) * c(4, 5, 6, 7, 8)  # 4, 10, 6, 14, 8
```

아래의 두 코드를 살펴보면 짧은 길이의 벡터가 긴 길이의 벡터에 반복해서 계산되는 것을 알 수 있습니다. 벡터는 여러 개의 요소 값(element value)을 가질 수 있는데, 그 중에서 하나의 값을 원할 경우에는 대괄호를 이용합니다.

```{r}
num_vec <- c(11, 21, 63, 44, 95, 86)
num_vec[3]        # 63이라는 값, 벡터의 세 번째 값을 얻게 됩니다.
num_vec[c(1,4)]   # c(1, 4)는 첫 번째와 네 번째의 값을 산출하라는 뜻으로 11, 44라는
                  # 결과를 얻게될 것입니다.
```

## 매트릭스 (Matrices)

이번에는 매트릭스를 살펴보겠습니다. 매트릭스는 일정한 수의 열과 행으로 이루어진 두 차원(two dimension)의 집합이라고 할 수 있습니다. 이때, 매트릭스를 이루는 요소들은 같은 유형의 자료들이어야 합니다. 매트릭스는 다음과 같은 함수를 통해 만들어볼 수 있습니다.

```{r}
matrix(1:12, byrow=TRUE, nrow=3)
```

매트릭스 함수를 이용하지 않고서라도 벡터들을 `cbind()`, 즉 열(column) 결합 또는 `rbind()`, 행(row) 결합 함수를 이용하여 합쳐 매트릭스를 만들 수 있습니다.

```{r}
c1 <- 1:3  # 1, 2, 3
c2 <- 4:6  # 4, 5, 6
c3 <- 7:9  # 7, 8, 9
cbind(c1,c2,c3)
rbind(c1,c2,c3)
```

매트릭스를 구성하는 요소 중 하나만 선택하기 위해서 우리는 대괄호(square brackets)를 사용합니다. 대괄호라고 하지만 이 과정은 **R** 프로그래밍에서 인덱싱(indexing)이라고 하는 것입니다. 즉, 행과 열의 목록에서 필요한 요소만을 지정해서 꺼낼 수 있도록 하는 기능입니다. 매트릭스는 두 차원으로 이루어져 있기 때문에, 특정 요소 하나만을 뽑아내기 위해서는 각각 차원에 배정된 숫자, 열 번호와 행 번호가 모두 필요합니다.

```{r}
matrix <- matrix(1:12, byrow=TRUE, nrow=3) # matrix라는 객체에 결과를 저장합니다.
matrix[1, 2]
matrix[1:2, 2:3]  # 더 작은 형태의 매트릭스로 추출되는 것을 확인할 수 있습니다.
```

기본적인 수학연산자를 이용하여 매트릭스도 요소들 간 계산을 할 수 있습니다. 

```{r}
11 + matrix #아까 저장해 둔 matrix 객체에 11을 더하면 모든 요소에 11이 더해집니다.
```

매트릭스를 이용한 계산을 자세하게 알고 싶다면 다음의 [링크](https://www.statmethods.net/advstats/matrix.html)를 참조하면 좋을 듯합니다. 

## 데이터프레임 (Data frame)

아마 사회과학 연구를 하게 되면 가장 많이 다루게 되는 자료 유형 중 하나일 것입니다. 일반적으로 우리가 사용하는 데이터셋은 거의 데이터 프레임 형태로 불러오게 됩니다. 데이터셋하면 일반적으로 행과 열이 있는 엑셀이 생각나 매트릭스랑 뭐가 다르지? 할 수 있는데, 아까도 말했다시피 매트릭스의 모든 요소는 동일한 유형의 자료여야 합니다. 정치학에서 많이 쓰는 자료 중 COW 데이터를 예로 들어보자면 COW 국가 코드(ccode)는 숫자형인 반면에, 국가 이름(cname)은 문자형입니다. 하나의 데이터셋에 서로 다른 유형의 자료가 담기게 되는 것입니다.

  + 국가명: 문자형
  + 국가의 GDP: 숫자형
  + 어떤 국가가 민주주의인지 여부: 논리형

데이터 프레임은 다양한 유형의 자료를 열과 행의 틀 안에서 저장할 수 있도록 돕습니다.


```{r}
# R에 내장되어 있는 데이터 프레임을 불러들여 보겠습니다
mtcars
```


데이터 프레임을 분석하는 데에는 여러 가지 이용가능한 함수들이 있습니다. 대표적인 것은 데이터셋의 상위 행 일부로 자료를 간략하게 보여주는 `head()`, 반대로 아래의 행들을 보여주는 `tail()`, 데이터셋이 몇개의 관측치(obs.)와 변수들로 이루어져 있는지 그 구조(structure)를 보여주는 `str()`, 요약통계치들을 제시하는 `summary()` 등이 대표적입니다.

만약 어떤 함수였는지, 어떤 자료 유형이었는지 헷갈린다면 **R** 콘솔 창에다가 물음표 뒤에 함수 이름을 쳐보면 자세한 정보를 확인할 수 있습니다.

그러나 데이터 프레임보다 자료를 불러올 때, `tidyverse` 패키지에 속한 티블 유형으로 불러올 것을 추천합니다.

## 티블 (Tibbles)

티블은 `tidyverse` 패키지의 속한 함수로 티블로 저장한 자료의 유형은 데이터프레임과는 약간 차이가 있습니다. 우선 티블의 장점은 더 유저 친화적이라는 것입니다. 예를 들어, 티블은 **R** 콘솔창에서 한 눈에 확인할 수 있을 정도로 데이터의 구조를 출력해주고, 각 열의 변수들이 가지는 자료 유형이 어떤 것인지를 보여줍니다. 종종 데이터프레임으로 구성된 자료 유형을 티블로 강제 변환해야 할 경우가 있는데, 이때는 `as_tibble()` 함수를 사용하면 된다.

일단 `tidyverse` 패키지는 **R**에 내장된 것이 아니라 Hadley Wickham이 개발한 것이기 때문에 별도로 불러와야 합니다. 패키지를 설치할 때는 `install.packages()`, 설치된 패키지를 불러올 때는 `library()` 혹은 `require()` 함수를 사용합니다.

```{r}
## install.packages("tidyverse") # 저는 이미 설치되어 있는 상태라 코멘트 처리합니다.
library(tidyverse)
mtcars <- as_tibble(mtcars)
mtcars
```

## 패키지 (Packages)

위의 `tidyverse` 패키지에 대한 설명이 사실 여기 들어와야 하는데, 티블을 설명하느라 조금 당겨서 적었습니다. 이 섹션에서 패키지에 대한 내용을 조금 더 자세하게 들여다 보도록 하겠습니다.

**R**의 패키지는 함수와 객체의 모음(collection of functions and objects)이라고 할 수 있습니다. *RStudio*나 **R**을 열때마다, 여러 개의 패키지들이 자동으로 로드(load)됩니다. 어떤 패키지들이 로드되어 있는지 확인하고 싶으면 `sessionInfo()` 라는 함수를 사용하면 됩니다.

좀 더 복잡한 문제를 해결하기 위해서는 **R**에 기본적으로 탑재된 함수/패키지 이외에 추가적인 패키지를 필요로 할 때가 있습니다. 이 경우에는 다음과 같은 함수를 사용합니다.: `install.package()`

그리고 **R**에서는 패키지의 설치와 사용은 별개의 작업으로 설치된 패키지를 사용하기 위해서는 그 패키지를 로딩해야 하는데 이때는 `library()` 함수를 사용하시면 됩니다. 한 번 패키지를 설치하면 다시 설치할 필요는 없지만 새로운 **R** 세션을 시작할 때마다 매번 `library()` 함수를 이용해서 로딩을 해주어야 합니다.

```{r, eval = FALSE}
## R 패키지 설치 예시
## foreign 함수는 version 12 이하의 STATA 파일(.dta)을 로딩할 수 있게 도와줍니다.
## install.packages("foreign")
## plyr 함수는 좀 더 복잡하고 고급스러운 자료 조작(manipulation)을 가능하게 합니다.
## install.packages("plyr")
## ggplot2 함수는 함수 가시화(visualization)를 돕습니다.
## install.packages("ggplot2")

# 설치한 패키지들을 사용하기 위하여 라이브러리(libraries)를 로드한다.
library(foreign)
library(plyr)
library(ggplot2)
```

이렇게 로드된 패키지들의 상태와 작업창을 한 번에 저장하고 다음에 불러오고 싶을 때, 다음과 같은 패키지를 사용해 패키지들을 관리할 수도 있습니다.

```{r, eval = FALSE}
## install.packages("session")
library(session)
save.session(file="test.Rda") # 현재까지 불러온 패키지와 객체들이 R 스크립트가 저장된
                              # 디렉토리에 test.Rda라는 이름으로 저장됩니다.
## 나중에 Rstudio 종료 후 다시 켰을 때,
restore.session(file="test.Rda") # 기존에 저장되었던 test.Rda를 불러옵니다.
## 이때, 주의해야할 점은 R 스크립트가 저장된 디렉토리가 세션 정보를 담은 Rda가 저장된
## 디렉토리와 같아야 한다는 점입니다. 만약 다르다면 file="다른 디렉토리/file.Rda"로 
## 별도로 지정해주어야 합니다.
```

패키지들에 대한 더 자세한 내용을 알고 싶다면 다음의 [링크](https://www.datacamp.com/community/tutorials/r-packages-guide)를 참고하면 좋습니다.

그리고 **R**이 여러 패키지를 설치하는 데 제약이 없다고는 하지만 로드는 개별 패키지별로 해야합니다. 마지막으로 한 번에 여러 개의 패키지들을 설치 및 로드할 수 있게 도와주는 패키지(패키지의 패키지...)가 있는데, 그건 [여기](https://gist.github.com/stevenworthington/3178163)에서 살펴볼 수 있습니다.[^2-3]

## 디렉토리 생성 코드

**R** 스크립트의 작성을 시작하기에 앞서, 디렉토리 생성 코드를 살펴보는 이유는 코딩하는 데 있어서 깔끔한 파일 구조를 설정하는 법을 숙지해야 효율적인 작업이 가능하기 때문입니다. 논문을 쓰는 입장이기 때문에 제 경우는 다음과 같이 폴더 구조를 정리합니다.

  + `Main project` directory $\leftarrow$ 예를 들어, [`2018_FALL_Regime_Growth`]
    + 폴더명 `code` subdirectory $\leftarrow$  **R** 스크립트를 여기다 저장합니다.
    + 폴더명 `tables` subdirectory $\leftarrow$  **R**에서 만든 표를 저장합니다.
    + 폴더명 `figures` subdirectory $\leftarrow$  **R**에서 만든 그래프 등을 저장합니다.
    + 폴더명 `tex file` subdirectory $\leftarrow$  논문 본문을 작성하는 `tex` 파일을 저장합니다.[^2-4]
    
하나 하나 윈도우 폴더 탐색기에서 만들 수도 있는데, **R**을 가지고도 편하게 만들 수 있습니다. 사실 익숙해져야 편하고 익숙해지기 전에는 약간 노가다 느낌나서 뭐하러 이짓하나 싶기도 합니다. 근데 익숙해지면 구조화된 폴더 트리 속에서 규칙적으로 네이밍되는 각 가지들로 이름만 바꾸면 되기 때문에 굉장히 편하다는 것을 알게 되실 겁니다.
  
  + 예를 들어서, 원래 하던 프로젝트가 `Main`이라는 폴더의 `P1`이라면 그 다음은 `P2`니까 `P1`의 **R** 스크립트에서 디렉토리를 `P2`로 주소를 바꾸기만 하면 됩니다.

```{r, eval = FALSE}
## 현재 R 콘솔에 저장된 모든 값, 모델 등을 제거하는 코드
rm(list=ls())

## 현재 작업중인 디렉토리가 어딘지 확인하는 코드
getwd()

## 새롭게 작업 디렉토리를 설정하는 코드
## 작업하고자 하는 폴더 우클릭 후 경로보기 하면 나옴
setwd("/Users/Documents")

## 표와 그래프를 위한 폴더를 만들기
dir.create("./tables")
dir.create("./figures")
```

## 용례 (A working example)

아래는 실제로 코딩을 이용해서 자료를 요약하거나 가시화하는 사례들입니다. 앞으로는 다음과 같은 내용들을 차근차근 다루어볼 것입니다.

```{r}
## 먼저 깔끔하게 R-콘솔 창을 정리합니다.
rm(list = ls())

## diamonds라는 데이터셋을 로드합니다. 이 데이터셋을 불러오려면 먼저 ggplot2를 설치하고
## 로드해야 합니다. ggplot2라는 패키지에 포함된 예제 데이터셋이기 때문입니다.
## install.packages("ggplot2") # 저는 이미 설치되어 있습니다.
library(ggplot2)
data(diamonds)
names(diamonds) # 데이터셋에 포함된 변수들의 이름을 확인할 수 있습니다.
head(diamonds) # 맨 위 몇 개 행의 특성을 간략하게 보여줍니다.
str(diamonds) # 데이터셋의 구조(관측치의 수, 변수의 수, 자료유형 등)를 보여줍니다.
summary(diamonds) # 데이터셋의 요약통계치(평균, 중간값, 분위수 등)를 보여줍니다.

## 라벨을 포함한 R 히스토그램
hist(diamonds$carat, main = "Carat Histogram", xlab = "Carat")

## R애 내장된 기본 함수가 아니라 ggplot2를 이용해서 똑같은 히스토그램 만들어 보겠습니다.
ggplot(data = diamonds) + geom_histogram(aes(x = carat))

## ggplot2는 "+"를 이용해서 다양한 형태의 추가적인 정보를 레이어 형식으로 더할 수 있습니다.
ggplot(data = diamonds) + 
  geom_histogram(aes(x = carat), fill = "grey50") + # 히스토그램 막대색 변경
  ylab("Frequency") + xlab("Carots") +
  ggtitle("Count of diamonds by size") +
  theme_bw() # 그래프 배경색 변경

## 아까 만들었던 그래프 폴더에 그래프를 저장할 수 있습니다.
# ggsave(file="./figures/figure1.pdf", width=6.5, height=5)
# ggsave(file="./figures/figure1a.png", width=6.5, height=5, device = "png")
```
```{r, eval = FALSE}
## 표 폴더 만든 것에다가 요약통계표를 저장하기
library(stargazer) # 통계표를 작성하는 데 특화된 패키지입니다.

## 세 가지 변수에 대해 요약통계치를 확인하기
diamonds <- subset(diamonds, select = c("carat", "depth", "price"))
## 몇몇 R 예제 데이터들은 티블로 저장되어 있지 않을 수 있습니다. 
## 이 경우에는 자료를 먼저 티블 유형으로 바꿔주고 시작하는 게 좋습니다.
## 자료 유형을 확인하는 함수는 class(), 혹은 typeof()입니다.
## library(tidyverse) # 아까 불러왔지만, 여기서는 로드하지 않았다고 가정합시다.
class(diamonds)
diamonds <- as_tibble(diamonds)
sum.table1 <- stargazer(diamonds, 
                      covariate.labels=c("Size (carats)", 
                                         "Cut", "Color", 
                                         "Clarity"), 
                                         title = "Summary stats for diamond data", 
                                         label = "table:summary1")
# write(x=sum.table1, file="./tables/Summary1.tex") # LaTex로 열고 편집할 수 있습니다.
```

## 기타 참고자료

여기서 정리한 `Introduction to R`의 내용은 모두 [DataCamp](https://www.datacamp.com/)와 `R for Everyone`이라는 자료의 내용을 요약, 정리한 것입니다. 책(`R for Everyone; R4E1`)이야 구매할 수밖에 없지만 `DataCamp` 사이트의 강의들 중에는 무료강의가 많으니까 한 번쯤 확인해보는 것도 큰 도움이 될 거라고 생각합니다.

RStudio 홈페이지도 두 개의 기초강의를 제공하는데, [RStudio Cloud](https://rstudio.cloud/)에 가입하여 `Primers`를 클릭하면 됩니다. `Primers`에서 `Basics`를 선택하면 두 개 강의를 볼 수 있는데, `R-coding`에 도움이 되는 건 `Programming Basics Course`입니다.


[^2-1]: 그에 관련된 포스팅은 [링크](https://blog.revolutionanalytics.com/2012/07/a-big-list-of-the-things-r-can-do.html)를 참조하시기 바랍니다.
[^2-2]: 참고로 R은 한글로 변수를 입력하는 기능을 제공하지 않습니다.
[^2-3]: Github는 알아두면 굉장히 유용한데, 이 내용은 나중에 차차 업로드하도록 하겠습니다.
[^2-4]: `.tex`의 확장자를 갖는 `LaTex` 혹은 `knitr` 패키지와 같은 맥락으로 문서작업에 유용한 `Rmarkdown` 등에 관한 정보는 나중에 따로 업로드하도록 하겠습니다.

<!--chapter:end:01-intro2R.Rmd-->

# Introduction to Data {#ch3}

여기에서는 자료를 관리하고 다루는 방법들에 대해 간단한 소개를 하고자 합니다. 구체적으로 다룰 내용들은 다음과 같습니다.

  + 무작위 추출 (Random draws)
  + 루프 (Loops)
  + 히스토그램 (Histograms)
  + 표 (Tables)
  + 벡터에서 요소들을 만들고, 계산하고, 추출하는 법 (Creating, summing, pulling elements from vectors)
  + 분포와 확률에 따라 사고하는 법

먼저, 시작하기에 앞서서 작업 디렉토리 (Working Directory)를 설정할 필요가 있습니다.

## 작업 디렉토리 설정

기본적으로 **R**은 설치할 때, 기본 설정된 폴더에 이어서 작업을 진행합니다. 이 경우 기존에 만들어놓은 `R.data` (**R** 파일 확장자)들을 불러들여서 Global Environment에 사용하지 않을 데이터들이 쌓이게 됩니다. 그러면 현재 작업하여 저장한 새로운 객체들과 기존 디렉토리에 상존하는 데이터들이 혼재되어 헷갈릴 수 있습니다. 

그리고 어차피 연구를 진행하면 해당 프로젝트에 따르는 폴더들을 만들어서 별개로 진행해야할 필요가 있기 때문에, 애초에 연구에 필요한 R.data를 만들 때 그 폴더에 만들고 작업 디렉토리도 설정하는 게 마음 편합니다.

그럼 일단 작업 디렉토리를 확인하고, 변경하는 코드를 살펴보겠습니다.

```{r, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(error = FALSE, warning = FALSE, message = FALSE,
                      fig.height = 3, fig.width = 6, fig.align = 'center')
```

```{r, eval = FALSE}
getwd() #현재 R이 인식하고 있는 작업 디렉토리가 어딘지 알려준다.
setwd("C:/Users/phere") #원하는 장소로 디렉토리를 변경한다.
```

기서 주의해야 할 것은, 디렉토리 주소를 적을 때에는 반드시 `""` 기호를 사용해야 한다는 것입니다. 그리고 아마 윈도우 PC는 `/`로 디렉토리를 구분하는 반면에 MAC OS의 경우는 `\`(back slash)를 사용하는 것으로 알고 있습니다. 

나중에 한 번 다루긴 하겠지만 몇 가지 함수들의 경우에는 MAC에서는 오류가 안 생기는데 윈도우에서는 오류가 발생하는 것들이 있습니다. 그런 경우가 조금 번거로울 때가 있기는 한데, 그 이외에는 뭐 다 같은 컴퓨터에 같은 **R**이니 큰 불편함은 없다고 할 수 있습니다 (하지만 MAC 구매 뽐뿌가 오는 것은 사실입니다).

```{r, eval = FALSE}
dir.create(path= "figures")
dir.create(path= "tables")
dir.create(path= "datasets")
dir.create(path= "references")
dir.create(path= "tex")
```

저 같은 경우에는 어떤 연구 프로젝트를 시작할 때, 제일 먼저 R-script 빈 것을 만들어두고 작업 디렉토리를 설정합니다. 

  + 이후에 위의 코드와 같이 세부 폴더들을 만듭니다. 
  + **R**로 만든 그래프들을 저장할 `figures`, 표를 저장할 `tables`, 기타 다운받은 데이터들을 저장할 `datasets`, 그리고 참고문헌을 저장할 `references`와 본문 작성을 위한 `tex` 폴더로 구성됩니다.
  
## 무작위 추출(Random Draws)

무작위 추출을 실제로 해보기 위해서 한 가지 시뮬레이션을 돌려 보겠습니다. 어떤 사업장에서 사람을 승진시키는 데 있어서 성차별(gender discrimination)이 있을 수 있다고 가정하는 것입니다. 간단하게 100명의 승진 후보자들이 있다고 가정하고, 남녀가 동일한 비율로 나뉘어져 있다고 생각해 보겠습니다. 그리고 일반적으로 승진할 확률은 70% (0.7)라고 가정합니다. R-code로 남녀 각각 50명을 대상으로, 승진할 경우 1로 코딩하고 승진하지 못할 경우를 0으로 코딩하는 일종의 더미변수를 만들겠습니다. 그리고 승진 확률은 70%로 설정합니다. 

  + 즉, 무작위로 추출한 50명 중 승진확률이 그대로 반영된다면 우리는 35개의 1과 15개의 0을 확인할 수 있을 것입니다.
  + 그러나 표본추출과 무작위화의 본연의 속성 상, 항상 확률대로 정확하게 그러한 비율의 결과를 갖는 것은 불가능합니다.
  + 따라서 우리는 무작위로 추출할 때마다 약 70% 승진확률에 근거한 그 언저리의 값들을 얻게 될 것입니다.

    ```{r}
MP <- rbinom(50, 1, .7) #70%의 승진확률로 무작위로 추출한 50명의 남성
WP <- rbinom(50, 1, .7) #70%의 승진확률로 무작위로 추출한 50명의 여성
sum(MP) #총 승진한 남성의 수
sum(WP) #총 승진한 여성의 수
sum(MP)/50 #전체 남성 승진후보자에 대한 승진한 남성의 비율
sum(WP)/50 #전체 여성 승진후보자에 대한 승진한 여성의 비율
    ```

`MP`와 `WP`는 각각 Mem Promotion과 Women Promotion의 약자입니다. `rbinom()`은 randomly draw as binomial로 이해하면 됩니다. 따라서 이 함수의 뜻은 "50개의 이항변수를 만드는 데 1의 값을 가질 확률을 0.7로 해서 추출해라"고 할 수 있겠습니다.[^3-1]

이렇게 만든 `MP`와 `WP`에 1, 즉 승진자가 각각 몇명인지 살펴보려면 단순하게 `sum()`, 합계를 나타내는 함수를 이용하면 됩니다. `MP`와 `WP`는 각각 벡터 자료로 값을 가지는데, `sum()`을 이용하면 이 벡터 자료의 각 요소들(elements)의 합을 계산할 수 있습니다. 만약 실제로 관측된 50명 각각에 대한 승진확률을 구하려면 그 집단의 총 인원수로 나누면 됩니다.

우리가 알고 싶은 것은 과연 이 사업장에서 승진하는 데 남녀의 성차별이 있느냐는 것입니다. 

  + 다시 말하면 남자와 여자의 승진 결과에 있어서 어떤 차이가 나타날 수는 있는데, 과연 그 차이가 진짜로 차별이 있어서 나타나는지를 확인하자는 것입니다. 
  + 이를 위해서 우리는 주어진 표본(남녀 각 50명, 총 100명에 있어서의)에서의 승진자 수의 차이를 계산해야 합니다.

```{r}
DP <- sum(MP) - sum(WP) #DP는 Difference in Promotion, 승진자 수의 차이입니다.
```

`DP`의 값이 양수(positive)라면 남성 승진자의 수가 더 많다는 의미일 것이고, 음수(negative)라면 여성 승진자의 수가 더 많다고 볼 수 있습니다. 만약 `DP`가 0이면 두 성별에서의 승진자의 수가 동일하다는 것입니다다. 우리는 **R**을 이용하여 이와 같은 표집(sampling)을 여러 번 시뮬레이션 해볼 수 있습니다. 

앞선 100명(남50 여50)을 추출한 것을 말 그대로 여러 번 반복할 수 있다는 것인데, 50명에 대한 확률은 0.7로 동일하더라도 무작위 추출이기 때문에 매번 추출할 때마다 그 결과는 달라질 것입니다. 

  + 우리가 알고 싶은 것은 이렇게 여러 번 추출해서 돌리더라도 만약 성차별이 실제로 존재한다면 꾸준하게, 그리고 일정한 차이로 `DP`가 나타날 것이라는 점입니다.
  + 만약 `DP`가 있다가 없다가 한다면 통계적 관점으로 "평균적으로" (on average) 그 효과는 체계적이지 않을(non-systematic) 가능성이 높습니다.

먼저, 10개의 결측치를 가진 벡터를 만들어보겠습니다. 이게 무슨 말이냐면 아무 값도 들어있지 않은 10개 열자리 1개 행의 표를 만들라는 얘기로 이해할 수 있습니다. 완전 같은 말은 아닌데, 뭐 이렇게 이해하면 편할 거 같습니다. 그리고 그 각각의 칸에 이제부터 총 10번 시뮬레이팅하여 남50 여50에 대한 승진자 수의 차이 값 10개를 채워넣겠습니다.

```{r}
trial.size <- 10 #시뮬레이션을 시도할 횟수
Test10 <- rep(NA, trial.size) ##rep는 replicate, 즉 반복하라는 함수입니다.
                              ##즉, 이 함수는 trial.size의 수만큼 NA를 반복해서
                              ##Test10이라는 벡터에 담으라는 의미입니다.
Test10 #위의 함수를 통해 얻게 되는 Test10 벡터의 값은 아래와 같습니다.
```

그리고 나서 우리는 이 10칸의 NA, 결측치(missing values)에 10번에 시뮬레이션을 통해 얻은 승진자 수의 차이(총 10개의 차이)를 담을 것입니다. 귀찮게 함수를 10번 반복할 수도 있겠지만, 만약 시뮬레이션을 해야 하는 수가 10번이 아니라 100만 번이라면 그 짓하다가 하루? 일주일이 다 갈 것이기 때문에 여기서부터 루프 (Loops)를 알아봅시다.

## 루프-반복(Loops)

자, 10번 반복하기를 해보겠습니다. 함수는 아래와 같습니다.

```{r}
for (a in 1:trial.size) { # a라는 벡터에 1부터 trial.size까지의 수를 넣으라는 명령
  MP <- rbinom(50, 1, .7) # MP를 계산하라, 총 50명이 0.7의 확률로 1을 가질 것
  WP <- rbinom(50, 1, .7) # WP를 계산하라, 총 50명이 0.7의 확률로 1을 가질 것
  Test10[a] <- (sum(MP) - sum(WP))/50
}
```

위의 함수를 통해서 우리는 총 10개의 `DP`값을 50으로 나눈, 승진자 수의 차이가 한 개 집단에(남 or 녀) 대해 비율로 계산된 벡터의 형식으로 `Test10`에 가지게 됩니다. `Test10[1]`은 첫 번째 시뮬레이션의 `DP` 확률이고 `Test10[4]`는 네 번째 시뮬레이션의 `DP` 확률을 가지게 될 것입니다.  벡터 자료 뒤에 `[n]`는 그 벡터의 `n`번째 요소를 보여달라는 명령입니다.

```{r}
hist(Test10)
```

총 10개의 `Test10`의 값을 구해 히스토그램을 구한 것입니다. 즉, 10개 `DP` 확률의 분포를 나타낸 것이라고 할 수 있습니다. 10개의 표본으로는 뚜렷한 경향을 보기가 힘듭니다. 한 번 백 개의 시뮬레이션을 진행해보겠습니다. 진행과정은 10번의 시뮬레이션이랑 동일합니다. 단지 `trial.size`가 100으로 늘어났다는 것과 반복횟수가 `b`로 `a`랑 구분한다는 것만 다릅니다.

```{r}
trial.size <- 100
Test100 <- rep(NA, trial.size)
for (b in 1:trial.size) {
  MP <- rbinom(50, 1, .7)
  WP <- rbinom(50, 1, .7)
  Test100[b] <- (sum(MP) - sum(WP))/50
}
hist(Test100)
```

매우 눈에 익숙하고 우리가 사랑하는(?) 분포의 형태로 변해가는 것을 볼 수 있습니다. 이제 극단적으로 백 만번의 시뮬레이션을 진행해보겠습니다.

```{r}
trial.size3 <- 1000000
Test1M <- rep(NA, trial.size3)
for (c in 1:trial.size3) {
  MP <- rbinom(50, 1, .7)
  WP <- rbinom(50, 1, .7)
  Test1M[c] <- (sum(MP) - sum(WP))/50
}
hist(Test1M)
```

여기서 한 가지 알 수 있는 것은 시뮬레이션 시도 횟수가 늘어날수록(무한에 가까워질수록), 우리가 알고 싶어하는 승진에서의 차이(결과)의 진짜 확률(true probabilities)이 드러난다는 것입니다.

  + 이는 나중에 표집(sampling)과 표집오차(sampling errors), 그리고 중심극한정리(Central Limit Theorem)을 언급할 일이 있을 때 다시 살펴보도록 하겠습니다.
  
위에서 추출한 100만번의 시뮬레이션 결과를 토대로 남성과 여성의 승진에 있어서의 차이가 과연 0.3보다 큰지 살펴보겠습니다. 

  + 조금 더 문제를 단순화하기 위해서 남자가 여성보다 더 승진할 가능성, 승진자 수의 차이가 전체 후보자에 대한 비율에 있어서 +0.3보다 큰지 작은지를 살펴보려는 것입니다.
  + 먼저 각각(남성과 여성)에 있어서의 승진자 수의 차이를 비율로 구하여 그 비율이 0.3보다 크거나 같은 경우가 전체 시뮬레이션으로 나타난 승진자 수의 차이 비율 전체의 몇 퍼센트를 차지하는지 구합니다.
    ```{r}
    length(Test1M[Test1M >= 0.3]) / length(Test1M)
    ```
  + 처음에 R을 할 때 잘 외우지 못했던 함수 중 하나인데, `length()`는 뭐랄까... `count`라고 이해하면 조금 더 쉬울 것 같습니다.[^3-2]
  + 즉, 아래의 함수는 `Test1M`, 100만개의 승진자 수가 전체 집단에서 차지하는 비율이 0.3보다 큰 경우만을 구해서 그걸 전체 승진차이 확률 100만개에 대한 비율로 구하라는 것입니다.
    + 남자가 여자에 비해서 30% 이상 승진을 많이 할 확률이 나타난 것이 총 100만번의 시뮬레이션 가운데 몇 번이었냐는 것입니다. 
  + 아래 함수는 그 결과는 0.000711, 즉 약 0.07%의 확률로 남자가 여자에 비해 30% 이상 승진자가 많을 확률이 나타난다는 것을 알 수 있습니다. 
  + 적어도 이 예제에 한하여 해석은 개인의 몫으로 남겨 놓겠습니다.

## Rplots를 파일의 형태로 저장하기

이번 섹션에는 조금 기술적인(technical) 소개를 하고자 합니다. 바로 위에서 만든 히스토그램 등을 `pdf`나 `png`와 같은 형태의 파일로 저장하는 함수입니다. 먼저 `pdf`로 저장하는 것을 살펴보겠습니다. `pdf`로 저장하는 것은 추천할만한 방식입니다. \LaTeX를 이용하는 경우에는 이렇게 저장한 pdf를 깔끔하게 \LaTeX로 생산하는 pdf 문서에 삽입할 수 있습니다.

```{r, eval = FALSE}
pdf("figures/histogram.pdf", width=8, height=6) 
## 아까 만든 figures에 histogram.pdf라는 이름으로 저장하게 합니다.
hist(Test1M, xlab = "Margin", main = NULL)  #표 전체 제목은 NULL, 없습니다
##Test1M, 100만번 시뮬레이팅한 히스토그램 X축에는 Margin이라고 레이블을 달 것입니다.
dev.off()
```

`dev.off`는 `device off`로 위의 함수는 여기서 끝!이라고 이해하시면 되겠습니다다.자세한 내용은 `?dev.off`로 알아보시길... 그리고 pdf로 저장하기 어려운 경우에는 png로 저장할 경우 해상도 자체는 조금 떨어지지만 유용하게 쓸 수 있는 그림파일로 동일한 Rplots를 저장할 수 있습니다. pdf는 인치(Inches)로 저장되는데, png는 픽셀(Pixels)로 저장됩니다. 그래서 아래 코드에서의 너비랑 높이를 지정하는 방법이 조금 다릅니다.

```{r, eval = FALSE}
png("figures/histogram.png", width=720, height=480)
hist(Test1M, xlab = "Margin", main = NULL)
dev.off()
```

## 데이터 다루기 기본

이번 섹션부터는 데이터에 대해 조금 더 깊이 살펴보고자 합니다. 구체적으로는,

  + 데이터 불러오기 (loading)
  + 서로 다른 분석수준, 분석단위를 가지고 작업하기 (working with different levels of analysis/units of observation)
  + 작업 흐름(workflow) 
  
순으로 진행하고자 합니다.

먼저 기존에 존재하던 데이터셋을 불러오기에 앞서 개략적으로 데이터라는 것이 어떻게 생겼는지를 살펴볼 필요가 있습니다. 데이터프레임은 열에 변수(variables), 행에 관측치들을 갖는 형태로 이루어져 있습니다. 하지만 앞서 언급했던 바와 같이, 데이터프레임 유형보다는 티블을 사용하는 것이 앞으로 배울 함수들을 적용하기에 좀 더 효율적입니다. 따라서 티블 함수를 이용해 가상의 데이터셋을 직접 만들어 보겠습니다.

```{r}
library(tidyverse) # 티블을 사용하기 위해서는 tidyverse 패키지를 불러와줘야 합니다.
data1 <- tibble(name = c("Jane", "John", "Jen", "James"),
                height = c(60, 70, 65, 68),
                eye.color = c("blue", "blue", "brown", "brown"),
                gender = c("female", "male", "female", "male"),
                highest.degree = c("college", 
                                   "high school", 
                                   "post graduate", 
                                   "college"))
glimpse(data1)
```

만약 티블이 아니라 데이터프레임으로 저장하고 싶으시다면 `tibble()` 대신 `data.frame()` 함수를 사용하시면 됩니다. 이렇게 생성된 데이터는 열(column)에 변수를 갖습니다. 

  + `data1`에서는 `name, height, eye.color, gender, highest.degree`가 변수명이 됩니다. 
  + 그리고 각 변수의 하위에 행마다 관측치들이 주어집니다다.
    + `name`이라는 변수에는 `Jane, John, Jen, James` 라는 각 개인을 관측한 결과가 입력됩니다다.
    + `c()`는 안의 요소들을 벡터의 형태로 묶으라는 것입니다(이전 포스팅에서 벡터에 대한 설명 참조). 
  + 그리고 이렇게 만들어진 데이터의 구조를 확인하기 위해서는 `glimpse()` 함수를 사용하면 된다.
    + `str()` 함수도 있는데, `glimpse()`가 좀 더 깔끔하게 데이터의 구조를 보여주는 것 같습니다.

### 데이터에 새로운 변수 & 더미변수 만들기

기존 데이터에 새로운 변수를 추가하는 방법은 매우 간단합니다. 그냥 새로운 변수명을 `$` 표시를 이용해 데이터에 써주고 거기에 배정(assign)을 의미하는 `<-` 표시로 넣어주면 됩니다. 백문이 불여일견이니 한 번 해보겠습니다.

```{r}
data1$female <- NA
```

위의 코드는 `female`이라는 변수를 새롭게 만들되 `female`의 모든 관측치는 결측치(missing values)로 생성하라는 것을 의미합니다.우측 항에 단순한 값을 적는다면 데이터의 새로운 변수, 일종의 벡터에는 모두 그 값으로 채워질 것입니다. 반면, 우리가 원하는 체계적인 형태의 변수(`var`iable)로 대체하고 싶다면 함수(function)를 이용하면 됩니다.

여기서 살펴볼 더미변수란 말 그대로 더미(dummy), 바보 변수입니다. 더미변수는 존부(存否)만을 나타내는 변수인데 `1`일 경우에는 있음, `0`일 경우에는 없음을 나타냅니다. 예를 들어, 바로 아래에서 만들 `female `변수는 여성일 경우에 `1`, 남성일 경우에 `0`을 나타낼 것입니다. 아까 만든 데이터(`data1`)의 변수 중 `gender`는 남성(`male`)과 여성(`female`)이라는 두 문자열 변수로 구성되어 있는데, 이를 숫자형(numerical)으로 바꿔주고자 하는 것입니다. 

더미변수는 어떤 점에서는 유용하지만 존부 이외의 자세한 정보를 제공하지 못한다는 점에서 더미라고 불립니다. 일단, 더미변수(숫자형)으로서의 여성(사실상 성별) 변수를 추가해보겠습니다.

```{r}
data1$female <- ifelse(data1$gender == "female", 1, 0)
## 해석: data1에 female이라는 변수에 우측 함수에 따른 값을 배정하라.
##      ifelse(만약 ~ 면, A를, ~가 아니라면, B를) 배정하라.
## 따라서 위의 함수는 data1의 gender 변수가 "female"이라는 문자일 경우 새로운 female
## 변수에 1을, "female"이 아닌 경우에는 0을 주어라.
data1$female # 더미 변수의 이름을 지을 때에는 기준값(reference value)이 헷갈리지 않게
             # 1의 값을 갖는 라벨(label)로 변수 이름을 짓는게 좋다 (TIP)
data1$gender <- NULL # 이제 사용하지 않을 gender 변수는 결측치로 변경.
```

그리고 한 가지 짚고 넘어갈 것은 **R**에서는 요인형(factor)과 문자형(character) 유형이 다르다는 것입니다. 요인형 자료를 문자형 자료로 변환하거나 혹은 그 역도 가능하지만 요인형에서 문자형으로 변환하는 것은 별로 추천드리고 싶지 않습니다. 일단 기본적으로 이 내용은 일반적인 통계분석을 다루고 있기 때문에 문자형 그 자체를 가지고 뭔가를 분석하는 텍스트 분석이 아닌 이상 문자열은 그저 고유값, 이상도 이하도 아니기 때문입니다. 하나 밖에 없는 값으로 무언가를 일반화하거나 설명하기란 쉽지 않습니다.

```{r}
data1$name <- as.character(data1$name) 
# name 변수의 자료들은 문자형(STATA에서 string이라고 하는)으로 이루어져 있는데, 
# 이를 요인형으로 바꾸는 것이다.
glimpse(data1)
knitr::kable(summary(data1))
```

위에서 요인형을 문자형으로 바꾸는 함수는 바로 `as.factor()`입니다. 직관적인 함수인데, 괄호 안의 변수를 요인으로써(as factor) 취급하여 다시 저장하라는 의미라고 볼 수 있습니다. 요인을 문자형으로 바꿀 수 있는 것처럼 요인형 변수를 숫자형 변수로 바꿀 수도 있습니다. 요인형 $\rightarrow$ 숫자형 변환 과정은 두 단계로 이루어집니다. 일단 예제 데이터를 만들어보겠습니다.

```{r}
GPA <- c("3.0", "4.0", "3.8", "2.2")
## 만약 "" 인용부호를 제외하고 벡터로 입력하면 GPA는 숫자형 자료가 될 것입니다.
## 기존의 data1 데이터프레임에다가 방금 만든 GPA를 새로운 열로 추가해보겠습니다.
data1 <- cbind(data1, GPA) #cbind는 열로 묶으라는 것입니다, 행으로 묶는 것은 rbind()
glimpse(data1)
knitr::kable(summary(data1))
```

요인형 변수(`GPA`)를 만들고 기존 데이터에 추가했으니, 이제 이 변수를 숫자형으로 바꿔겠습니다. 앞서 언급했다시피 이 과정에는 두 가지 단계가 요구됩니다.

```{r}
data1$GPA <- as.numeric(as.character(data1$GPA))
names(data1)[6] <- "GPA.num" # 기존 요인형 GPA랑 새롭게 만든 숫자형 GPA 비교를 위해
                             # .num(numeric 약자)을 붙여 새로운 변수로 만듭니다.
table(data1$GPA.num)
```

`names(data1)[6]`은 데이터프레임의 여섯 번째 열에 이름을 지어라(names)라는 코드입니다. 그 이름을 `GPA.num`으로 하기 위해 `<- "GPA.num"`이 지정되었습니다.  만약 요인변수를 직접적으로 숫자형으로 바꾸고자 시도할 경우에는 문제가 생길 수 있습니다. 아래의 코드를 보겠습니다.

```{r}
data1 <- cbind(data1, GPA)
data1$GPA <- as.numeric(data1$GPA) # 이렇게 하면 문제가 생김
glimpse(data1$GPA)
```

두 방법의 차이를 알시겠나요? `GPA` 변수의 소수점이 다 사라지고 정수형으로 바뀌어버렸습니다. 이것이 요인형을 숫자형으로 바꾸는 데 두 단계가 필요한 이유입니다.

### 다른 유형의 데이터 불러오기 (Loading data in different formats)

간략한 데이터셋을 직접 만들어보았으니, 이번에는 다른 연구자/기관이 구축한 데이터를 불러오는 방법을 살펴보겠습니다. 이 포스팅에서 사용할 데이터셋은 2016년도 기준으로 측정된 국가 단위의 자료이다. 이 자료의 원출처는 다음의 [링크](https://qog.pol.gu.se/data/datadownloads/data-archive)에서 확인할 수 있으며, 본 포스팅에서 사용할 자료는 미리 분석을 용이하게 하기 위하여 일정 변수들만을 선별한 자료입니다. **STATA** 파일로 저장된 자료를 사용합니다.

한 가지 말해두자면, **R**은 여러 가지 과정과 방법들로 동일한 결과를 얻을 수 있기 때문에, 자신에게 보다 효율적인 방법을 찾아가는 것이 중요합니다.
```{r, echo = FALSE}
glimpse_head <- function(x, n = 10) {
  print(head(x, n))
  invisible(x)
}
```

```{r}
## STATA 파일을 불러오기 위해서는 "foreign" 패키지가 필요합니다.
## install.packages("foreign") # 저는 이미 설치가 되어 있습니다.
library(foreign) # 설치만 해서는 안되고 패키지를 불러와야 합니다.
## STATA 파일을 불러와 보겠습니다.
here::here() %>% setwd()
QOG <- read.dta(file = "example.data/qog_std_cs_jan19_ver13.dta", 
                convert.underscore = TRUE)
## foreign 함수로는 STATA 버전 13 이전의 자료만 불러들일 수 있습니다. 즉, 아래 코드는 불가.
QOG <- read.dta(file = "example.data/qog_std_cs_jan19_ver15.dta", 
                convert.underscore = TRUE)
## 그렇다면 버전 13 이후는 무슨 패키지를 사용해야 할까요?
## 버전 13 이후는 "readstata13" 로 불러올 수 있습니다.
# install.packages("readstata13")
library(readstata13)
QOG.v2 <- read.dta13(file = "example.data/qog_std_cs_jan19_ver15.dta", 
                     convert.underscore = TRUE)

## 또 다른 방법이 있다. 바로 "haven" 패키지를 이용하는 것입니다.
## install.packages("haven")
library(haven)
QOG.v3 <- read_stata("example.data/qog_std_cs_jan19_ver15.dta")

## 근데 저는 foreign 이나 haven 패키지 모두 안 씁니다.
## 더 효율적인 패키지를 찾았거든요. 바로 ezpickr 입니다.
## install.packages("ezpickr")
library(ezpickr)
QOG.v4 <- pick("example.data/qog_std_cs_jan19_ver15.dta")
```

### 자료 머징하기 (Using Data Merging)

자료 머지(merge)에도 여러 가지 유형이 있는데, 오늘 간단하게 살펴볼 것은 기존 데이터을 다른 데이터의 변수들을 이용해 확장하는 유형의 머징입니다.  두 국가 간의 인구 차이를 측정하는 국가쌍(dyadic) 변수를 코드하고자 한다고 해보겠습니다. 먼저 `QOG` 데이터의 하위 셋(subset)을 만듭니다.

```{r}
## names(QOG) # QOG 데이터프레임의 변수명을 나열하라는 함수입니다.
## 변수가 엄청 많습니다.
length(names(QOG)) # 1983개의 변수
QOG.tomerge <- subset(QOG, select = c(ccodecow, wdi.pop))
## QOG.tomerge라는 하위 셋을 만들라는 명령입니다.
## QOG라는 자료에서 ccodecow와 wdi.pop라는 두 변수만을 선택(select)하여 만듭니다.
QOG.tomerge <- subset(QOG, ccodecow > 100, select = c(ccodecow, wdi.pop))
## QOG.tomerge라는 하위 셋을 만들어라. 이 경우에는 앞의 QOG.tomerge를 대체(replace)합니다.
## QOG라는 자료에서 ccodecode가 100보다 큰 경우에 한하여(조건)
## ccodecow와 wdi.pop라는 변수를 선택하여 하위 셋을 만듭니다.
## 결과적으로 QOG.tomerge는 cowcode가 100보다 큰 국가들의 세계발전지표 상의 인구
## 지표들을 가지게 됩니다.
```

교차사례 데이터 `QOG.tomerge`를 중복하여 머지해보겠습니다. 이렇게 하면 우리는 총 `국가1`과 `국가2`, 그리고 연도에 따른 동일한 변수들을 갖는 결합된 데이터를 갖게 될 것입니다. 각각의 관측치들은 모든 관측치와 매칭(matching)이 되기 때문에 우리는 동일한 국가 쌍의 자료들을 갖게 됩니다. 말이 더 어렵군요... 백문이 불여일견.

```{r}
QOG.dyad <- merge(x = QOG.tomerge, y = QOG.tomerge, by = NULL)
```

![이렇게 머지하면, x 국가군의 cowcode와 인구지표, 그리고 y 국가군의 cowcode를 가지게 됩니다.](r.post/merge.png)

이 경우에 동일한 국가쌍(self-dyads)은 제거해야 두 국가 간의 관계를 살펴볼 수 있을 것입니다. 동일한 국가의 인구 지표는 차이가 없으니까요!


```{r}
names(QOG.dyad)
QOG.dyad <- subset(QOG.dyad, ccodecow.x != ccodecow.y)
## QOG.dyad 자료에서 ccodecow.x가 ccodecow.y와 다른 경우만 다시 저장합니다.
QOG.dyad$pop.dif <- QOG.dyad$wdi.pop.x - QOG.dyad$wdi.pop.y
## QOG.dyad 데이터프레임에 pop.dif, 인구차이라는 변수를 새로 만듭니다.
## 인구 차이는 x국가의 wdi.pop.x에서 wdi.pop.y 를 감한 값입니다.
## 이 변수는 x국가와 y국가 간 인구 차이, 즉 두 국가 간의 역동적 관계를 보여줍니다.
```

위와 같이 `wdi.dif`라는 인구 차이 변수는 사실 `x`에서 `y`를 빼나 `y`에서 `x`를 빼나 부호를 제외하고 그 크기는 동일합니다. 따라서 우리는 따로 방향(direction)을 고려할 필요가 없습니다. 아래의 코드는 방향성을 제거한 국가쌍 자료(non-directed dyads)를 만드는 것입니다.

```{r}
QOG.nddyad <- subset(QOG.dyad, ccodecow.x < ccodecow.y)
QOG.nddyad$pop.dif.nd <- abs(QOG.nddyad$wdi.pop.x - QOG.nddyad$wdi.pop.y)
## abs는 absolute value, 부호를 고려하지 않기 위해서 절대값으로 만들라는 것입니다.
```

이번에는 국가-연도(country-year) 자료를 국가쌍-연도(dyad-year) 자료로 전환하는 사례를 살펴보겠습니다. 이를 위해서 일단 교차사례 시계열 데이터셋을 웹에서 다운받아 열어봅니다(time-series cross-sectional dataset). 이렇게 불러온 데이터셋은 1816년부터 2016년 사이의 국가들의 자료를 구축한 국가-연도를 분석단위로 한 자료입니다.

```{r}
## COW 국가-연도 목록을 불러왔습니다.
stateyear <-
  read.csv("http://correlatesofwar.org/data-sets/state-system-membership/system2016",
                      head = TRUE, sep = ",")# Look at country codes
unique(stateyear$ccode) # 중복되지 않는 국가코드만 보이게 했습니다.
table(stateyear$ccode)  # 각 국가코드가 몇 개의 관측치를 가지는지를 보여줍니다.
```

불러온 목록에서 숫자형으로 저장된 `COW `국가 코드만 따로 떼어서 볼 수 있고, 코드별로 연도별 자료가 얼마나 관측되어 있는지 확인할 수 있습니다. 이제 여기서 연도와 `ccode` 변수만 따로 떼어보겠습니다.

```{r}
stateyear <- subset(stateyear, select = c(year, ccode))
```

그리고 새로운 패키지, `countrycode()`를 이용하여 국가 이름 변수를 새롭게 만들어 데이터에 추가해보겠습니다.

```{r}
## install.packages("countrycode")
library(countrycode)
stateyear$countryname <- countrycode(stateyear$ccode, "cown", "country.name")
## stateyear 자료에서 ccode 변수를 숫자형("cown")에서 문자형("country.name")으로 변경합니다.
## stateyear 자료에 countryname 이라는 새로운 이름 변수를 만들어 바꾼 자료를 배정합니다.

## 이 자료에서 결측치(missing values)를 제거해보겠습니다. 
## complete.cases는 결측치를 제외한 변수들 짝이 완전하게 맞는 사례들만을 선별하라는 옵션입니다.
stateyear <- subset(stateyear, complete.cases(stateyear))
head(stateyear)
```

연도 변수를 이용해서 이번에는 다시 국가쌍 자료로 머징해보겠습니다.


```{r}
dyadyear <- merge(x=stateyear, y=stateyear, by.x=c("year"), by.y=c("year"))
head(dyadyear)
```

아까처럼 1행의 자기자신의 쌍을 구성하는 경우를 제거해보겠습니다. 이번에는 `subset()` 함수가 아니라 조건(condition, if)를 의미하는 대괄호를 이용해보겠습니다.

```{r}
dyadyear <- dyadyear[dyadyear$countryname.x != dyadyear$countryname.y, ]
head(dyadyear)
```

과연 이렇게 만든 게 자기쌍(self-dyads)을 잘 제거했는지 확인해볼 필요가 있습니다. 논리형 연산자(`==`)를 이용하기 때문에 결과는 `TRUE` 또는 `FALSE`로 나타날 것입니다.

```{r}
table(dyadyear$countryname.x == dyadyear$countryname.y)
```

이번에는 1980년 이후의 자료들만 가지고 국가쌍의 하위 데이터셋을 구해보겠습니다. 대괄호(`[]`)와 `subset()` 함수를 이용한 방법 모두를 살펴보겠습니다.

```{r}
## 대괄호 [] 를 이용한 방법
dyadyearp80 <- dyadyear[dyadyear$year >= 1980,] 
glimpse(dyadyearp80)
## subset 함수를 이용한 방법
dyadyearp80 <- subset(dyadyear, dyadyear$year >= 1980)
glimpse(dyadyearp80)
```

이번에는 변수들의 이름을 바꿔보겠습니다. 첫 번째는 기본 함수를 이용하고, 두 번째는 `plyr` 패키지를 이용해서 동일한 결과를 만들어 보겠습니다.

```{r}
## 기본 함수를 이용해 변수명 바꾸기
## names(dyadyearp80)[변수 순서] <- "바꿀 변수 이름"
## 2번째 변수부터 5번째 변수들의 이름을 바꿔보자.
names(dyadyearp80)[2:5] <- c("ccode1", "countryname1", 
                             "ccode2", "countryname2")
head(dyadyearp80)
## install.packages("plyr")
library(plyr)
dyadyearp80 <- rename(dyadyearp80, c("ccode.x" = "ccode1", 
                                     "ccode.y" = "ccode2",
                                     "countryname.x" = "countryname1",
                                     "countryname.y" = "countryname2"))
```

### 예제: correlatesofwar.org에서 capabilities 데이터셋을 불러오기[^3-3]

```{r}
cinc.link <- 
  "http://correlatesofwar.org/data-sets/national-material-capabilities/nmc-v4-data"
cinc <-
  read.csv(file = cinc.link,
           head = TRUE,
           sep = ",",
           na = c(-9))
```

`COW` 홈페이지에서 `CINC` 데이터셋을 로드해보겠습니다. `CINC` (Composite Index of National Capability) Score는 국가-연도 별로 국가의 물질적 능력(national materials capabilities)에 대한 구성요소로서 측정된 여섯 가지 개별 지표들을 종합하여 한 지표로 만든 결과입니다 (Singer, Bremer and Stuckey, 1972). 

  + `CINC`는 각 구성요소를 동등하게 가중치를 부여하여 종합한 개별 연도마다의 능력 (capabilities)의 평균을 체계 전체 (total system)에서의 몫(share)으로 나타낸 것이다. 
  + 결과적으로 `CINC`는 0부터 1 사이의 값을 가지고 0은 해당 년도 체계 내에서 그 국가가 전체의 0%의 능력을 가지고 있다는 것을 의미합니다. 
  + 반대로 1은 주어진 연도에서의 100%의 역량을 보여줍니다 (Correlates of War Project National Material Capabilities (NMC) Data Documentation Version 5.0. Codebook 참조)

`CINC` 데이터셋을 `ID`와 `CINC score`의 두 변수만 갖도록 분할해보겠습니다.

```{r}
cinc.cut <- cinc[c("ccode", "year", "cinc")]
```

`CINC` 값은 주어진 연도에서 국가쌍이 아닌, **한 국가**의 힘을 측정한 결과입니다. 그러나 우리는 이 `CINC` score를 이용해서 국가쌍의 변수로 만들 것이기 때문에, 먼저, 첫 번째 국가군 (country1 of `ccode1`)에 대한 CINC 자료들을 머지해보겠습니다.

```{r}
dyadcap <- merge(x = dyadyearp80,
                 y = cinc.cut,
                 by.x = c("ccode1", "year"),
                 by.y = c("ccode", "year"))
dyadcap <- rename(dyadcap, c("cinc" = "cinc1"))
head(dyadcap)
```

국가쌍의 역량을 보여주는 변수(dyad capabilities)라는 의미로 `dyadcap`이라는 데이터를 만들어았습니다. 머지할 첫 번째 데이터는 이전에 만들어 둔 `dyadyearp80`이고 두 번째 머지 대상 데이터는 `cinc.cut`입니다. `merge()` 함수는 나중에 따로 구체적으로 다루겠지만 아래의 [그림]((https://www.rapidinsightinc.com/7-data-cleanup-terms-explained-visually/))에서 개념을 개략적으로 파악할 수 있습니다.

![](r.post/Merging.jpg)

그림을 보면 `State` 변수를 기준으로 같은 `State`에 `Capital`과 `Population`이 묶인 것을 볼 수 있습니다다. 위의 R-code에서는 첫 번째 데이터에서는 `ccode1`과 `year` 변수를 기준으로, 두 번째 데이터에서는 `ccode`와 `year`를 기준으로 `dyadyearp80`의 `ccode1`과 `year`를 `cinc.cut`의 `ccode`와 `year`를 동일한 것으로 간주하여 두 변수를 하나의 데이터셋 안에 머지하라는 의미입니다.

국가쌍의 `CINC` 변수를 만든다는 것은 국가1과 국가2의 `CINC` 변수 두 개가 필요하다는 것입니다. 그렇다면 이후에 또 머지하여 추가될 변수와 기존의 변수가 충돌하지 않기 위해서 이름을 바꾸어줄 필요가 있습니다. 그래서 첫 번째 `CINC` 변수를 `CINC1`로 바꾸어주었습니다.

이제 국가2의 `CINC` score를 머지해보겠습니다. 앞서와의 동일한 과정을 진행하되 `ccode2`에 대한 `CINC` score를 머지해야 하니까 기준을 바꿔주면 됩니다.

```{r}
dyadcap <- merge(x = dyadcap, 
                 y = cinc.cut, 
                 by.x = c("ccode2", "year"), 
                 by.y = c("ccode", "year"))
head(dyadcap)
dyadcap <- rename(dyadcap, c("cinc" = "cinc2"))
```

이렇게 만들어진 두 개의 `CINC` score를 이용하여 국가1과 국가2의 상대적 국력을 측정하는 변수를 새롭게 만들어 보겠습니다.

```{r}
dyadcap$caprat <- dyadcap$cinc1/dyadcap$cinc2
```

이렇게 만들어진 변수는(데이터는) 방향성을 가진 국가쌍의 분석 수준의 자료입니다. 예를 들어, 이 자료에는 미국-캐나다 간의 국력과 캐나다-미국 간의 국력이 순서만 바뀐 채로 동일한 값 (동일한 `caprat` 변수값)을 가지고 있습니다. 

이 중복된 값은 불필요하기 때문에 방향성을 제거할 필요가 있습니다(non-directed). 아래의 변수는 `ccode1`이 `ccode2`보다 작은 경우만을 남기라는 코드입니다. 즉, `ccode` 값이 2인 미국과 `ccode` 값이 731(맞나...? 이거 쓸 때는 지금 코드북을 안보고 있숩... 아마 우리나라 맞을 것인디...)인 한국의 상대적 국력 변수를 보면,

```{r}
dyadcap %>% select(ccode1, ccode2, year, caprat) %>%
  dplyr::filter(ccode1 %in% c(2, 731), 
                ccode2 %in% c(2, 731), 
                year == 2000) %>% 
  knitr::kable()
```

로 두 국가 간의 국력비교 변수가 중복되어 있음을 확인할 수 있습니다. 이를

```{r}
dyadcap %>% 
  dplyr::filter(ccode1 %in% 2, 
                ccode2 %in% 731, 
                year == 2000) %>% knitr::kable()
```

의 결과로 바꾸어주는 것입니다. 

```{r}
dyadcapnd <- dyadcap[dyadcap$ccode1 < dyadcap$ccode2,]
```

이번에는 방향성을 제거한 국가쌍의 상대적 국력 지표를 만들어보겠습니다. 두 국가의 국력 중 큰 국력을 작은 국력으로 나누어서 상대적 국력 변수를 만들 것입니다.

```{r}
dyadcapnd$caprat <- pmax(dyadcapnd$cinc1, dyadcapnd$cinc2)/
  pmin(dyadcapnd$cinc1, dyadcapnd$cinc2)
summary(dyadcapnd$caprat)
```

상대적 국력 변수라고 이름짓는 이유는 두 `cinc1`과 `cinc2`의 값이 같다면(상대적 국력이 같다면) 이 변수는 1의 값을 가질 것이고, 만약 `cinc1`이 `cinc2`에 비해 상대적으로 큰 국력을 가진다면 1보다 큰 값을 가질 것이기 때문입니다. 다만 `pmax()`와 `pmin()` 함수는 뒤에 포함된 괄호 안의 두 값 중 `최대값`과 `최소값`을 뽑아내라는 의미이므로 두 국가쌍의 값은 항상 최대값/최소값을 갖습니다.

만약 이 함수를 이용하지 않았더라면 `cinc1 < cinc2` 인 상황에서 `cinc1/cinc2`로 계산되어, 1보다 작은 분수 값을 가지게 되었을 테지만, `pmax & pmin` 조합을 통해서 우리는 1을 최소값으로 갖는 변수로 상대적 지표를 조작하게 된 것입니다.

## 요약통계표를 .tex 파일로 저장하기

```{r}
library(stargazer)
dyadcapnd <- subset(dyadcapnd, 
                    select = -c(countryname1, countryname2))
glimpse(dyadcapnd)
```

`= -c` 표시는 이 기호 뒤의 벡터를 제외한 모든 변수들을 선택하여 하위 데이터셋을 만들라는 명령입니다. 혹은 아래와 같이 포함하고 싶은 변수들을 벡터로 하나하나 다 적어도 됩니다.
`stargazer()` 함수를 이용해서 요약통계치를 만들 때에는 굳이 국가명이 필요하진 않으니 제외하였습니다.

```{r, eval = FALSE}
stat.table <- stargazer(dyadcapnd, 
                        covariate.labels = 
                          c("Country code 1", 
                            "Country code 2", "Year", 
                            "CINC 1", "CINC 2", 
                            "Capability ratio"), 
                        title = "Summary Statistics",
                        label = "stat.table")
write(x = stat.table, file = "tables/table1.tex")
```

.tex 파일에 대한 소개는 다른 탭의 포스팅을 통해 진행하도록 하겠습니다. 일단 여기서는 코드만 알아두고 써먹을 일 있으면 `copy & paste` 해보시기 바랍니다.

## 서로 다른 분석수준(lower & higher) 통합하기

일단 `COW`의 양자간 무역 데이터(Correlates of War trade bilateral data version 3.0)를 불러오겠습니다자. 이 변수는 csv 또는 dta 파일과는 달리 [온라인](http://www.correlatesofwar.org/COW2%20Data/Trade/Trade.html)에 게재되어 있기 때문에 바로 객체로 불러와 열 수는 없습니다. 따라서 먼저 압축파일(zip)의 형식으로 다운받고 난 뒤에 그 압축파일에서 필요한 자료를 추출해내야 합니다.

```{r}
link <- "http://correlatesofwar.org/data-sets/bilateral-trade/cow_trade_3.0"
download.file(url = link,
              destfile = "COWTrade3.0.zip", mode="wb")
unzip("COWTrade3.0.zip", exdir = getwd())
```

압축을 푼 파일을 **R**의 객체로 불러들여 열고 분석할 것입니다. 데이터는 압축이 풀린 폴더의 하위에 있기 때문에 그 디렉토리를 정확하게 반영하여 코드를 짜야 합니다.[^3-4]

```{r}
btrade <- read.csv(file = "COW_Trade_3.0/dyadic_trade_3.0.csv")
names(btrade)
glimpse(btrade)
```

이렇게 불러들여온 양자간 무역 자료의 분석수준은 무엇일까요? 그리고 각 변수들은 무엇을 의미할까요? 이러한 내용은 **R**로 불러들여온 자료만으로는 알 수 없습니다. 따라서 항상 데이터셋을 다운받을 때는 코드북을 같이 다운 받아서 숙지하는 습관을 들일 필요가 있습니다.  일단 당장의 분석에 필요없는 변수들을 날려버리겠습니다.

```{r}
btrade$source2 <- 
  btrade$bel_lux_alt_flow1 <- 
  btrade$bel_lux_alt_flow2 <- 
  btrade$china_alt_flow1 <- 
  btrade$china_alt_flow2 <- 
  btrade$version <- NULL
glimpse(btrade)
```

중요한 것은 이 데이터셋에서 결측치는 `NA`가 아니라 `-9`로 코딩되어 있다는 사실입니다. 만약 이 `-9`들을 가만히 놔뒀다가는 요약통계와 추론통계를 부정확하게 만들 것이기 때문에 `flow1`과 `flow2`의 결측치들을 `NA`로 바꾸어 주어야 합니다. 사실 csv 파일을 불러들여올 때 했을 수도 있지만 여기서는 불러들인 상황에서 뒤늦게 `-9`를 발견하고 바꾸는 대처 방법을 배워봅시다.

```{r}
btrade$flow1[btrade$flow1 == -9] <- NA # 대괄호는 조건(condition)을 의미합니다.
btrade$flow2[btrade$flow2 == -9] <- NA
summary(btrade$flow1) # -9이 사라졌습니다.
```

개별 국가들의 연간 총무역량을 계산해보겠습니다. 먼저 국가쌍의 수입(imports)과 수출(exports)을 더합니다. 이때 만약 `flow1`나 `flow2` 둘 중 하나가 결측치일 경우 총무역량 또한 결측치로 나타날 것입니다.  따라서 단순 더하기 코드로는 결과가 왜곡될 수 있습니다.

```{r}
btrade$tottrade1 <- btrade$flow1 + btrade$flow2 # 잘못된 결과를 얻게 됩니다.
summary(btrade$tottrade1)

btrade$tottrade2 <- ifelse(is.na(btrade$flow1), btrade$flow2,
                          ifelse(is.na(btrade$flow2), btrade$flow1,
                                 btrade$flow1 + btrade$flow2))
summary(btrade$tottrade2)
```

첫 번째 코드는 `flow1`과 `flow2` (수입과 수출)을 더하여 `총무역량1`이라는 변수를 만들라는 것입니다. 이때, 둘 중 하나에 결측치가 있으면 그 합 역시 결측치가 됩니다.

이 문제를 피하기 위한 두 번째 코드는 만약 `flow1`이 결측치라면 `총무역량2`라는 변수에 `flow2`의 값을 부여하라는 얘기이고 만약 `flow1`이 결측치가 아니며, 동시에 `flow2`는 결측치라면 그 자리에 `flow1` 값을 부여하라는 얘기입니다. 만약 둘 다 결측치가 아닐 경우에는 마지막의 조건, `flow1`과 `flow2`를 결합한 값을 넣으라는 얘기입니다. 

이와 같은 결과는 `plyr` 패키지를 이용해서도 얻을 수 있습니다. `plyr`패키지의 `summarise()` 함수를 이용하여 새로운 데이터프레임을 만들 수 있습니다. 물론 기존 데이터프레임으로 지정하면 새롭게 통합(aggregate)된 자료로 대체됩니다.

이렇게 얻은 `styrtrade1`이라는 변수는 `ccode1`과 `year` 단위로 결측치를 제외하고(`na.rm = TRUE`) 총무역값을 전부 더한 결과입니다. 그리고 그 양자간 무역 총액 변수를 머지하여 `btrade` 데이터에 붙여줍니다. 이제 `국가1`의 `특정연도` 하에서의 양자간 무역과 그 해의 총액을 확인할 수 있습니다.

아래의 코드로 만든 `share1`은 `국가1(state1)`의 총무역량에서 양자간 무역이 차지하는 비율을 보여줍니다.

```{r}
library(plyr)
btrade <- ddply(btrade, .(ccode1, year), 
                transform, styrtrade1 = sum(tottrade2, na.rm = TRUE))
btrade$share1 <- btrade$tottrade2 / btrade$styrtrade1 
```

오늘의 포스팅에서는 꽤나 여러 가지 자료들을 다루어 보았는데, 좀 복잡할 수도 있습니다. 사실 저도 IR 쪽 자료는 잘 사용하지 않고 교차사례 시계열로만 쓰는데 IR은 그걸 국가쌍의 자료로(dyadic) 재구성해서 국가들 간의 관계를 살펴보는 연구들도 합니다. 여하튼 이번 포스팅에서는 `plyr`패키지의 사용을 확인하고 숙지하는 것만 건져도 승리하는 것이라고 할 수 있겠습니다.



[^3-1]: 자세한 내용은 Kachitvichyanukul, V. and Schmeiser, B. W. (1988) "Binomial random variate generation." *Communications of the ACM*, 31, 216–222.를 참조. `?rbinom`이라고 R-console에 입력하면 우측 하단의 창에서 함수들에 대한 자세한 설명을 볼 수 있습니다.
[^3-2]: 나중에 가면 알아볼 `dplyr` 패키지에는 그룹별 관측치의 개수를 셀 수 있는 `count()` 함수가 존재합니다.
[^3-3]: 단, 결측치를 의미하는 -9를 모두 NA로 바꿔서 불러 들여올 것입니다.
[^3-4]: 정말 짜증나는군요. 이런 데이터는 써주지 말아야 합니다. `COW` 불매운동...

<!--chapter:end:02-Intro2Data.Rmd-->

# Probability into R

```{r, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(error = FALSE, warning = FALSE, message = FALSE,
                      fig.height = 3, fig.width = 6, fig.align = 'center')
```

확률 관련 포스팅에서는 몇 가지 간단한 확률을 **R**을 통해서 증명하고(Some simple probability demonstrations), 정규분포(normal distributions)와 이항분포(binomial distributions)로부터 분위(quantiles)를 얻어내는 방법을 살펴보고자 합니다. 

여기서는 일단 간단하게 정규분포랑 이항분포, 그리고 독립사건과 종속사건만 간단하게 살펴보겠습니다.

  + 분위(Quantiles)에 대해 어떻게 설명할까 고민을 좀 했는데 좋은 블로그 포스팅을 찾아서 참조용으로 링크하겠습니다.
  + 링크는 [여기](https://blog.naver.com/sw4r/221026102874)를 보시면 됩니다. Q-Q Plot 설명하면서 Quantile의 정의도 잘 정리해놓으셨습니다.

그럼 본격적으로 포스팅을 시작하기에 앞서서 항상 그렇듯이 작업 디렉토리를 설정해보겠습니다.

```{r, eval = FALSE}
rm(list=ls()) # 현재 콘솔 창에 저장되어 있는 모든 값과 모델 등을 삭제 
```
```{r}
library(here)
library(knitr)
library(dplyr)
library(tidyverse)
library(kableExtra)
here::here() %>% setwd()
```

## 주사위굴리기 게임!

확률을 공부할 때, 지겹도록 등장하는 놈들이 총 셋이 있습니다. 동전, 카드, 그리고 주사위입니다. 인류는 아마도 이 셋을 만듦으로써 스스로를 괴롭히는 통계학을 발전시켰는지도 모르겠습니다... 일단 주사위 굴리기는 직관적으로 확률과 통계를 이해하기 좋은 방식입니다. 먼저 주사위 하나를 한 번 굴리는 것을 시뮬레이팅하는 함수를 코딩해보겠습니다.

```{r}
die <- as.integer(runif(1, min=1, max=7))
die
```

`die`는 `?runif` 라고 입력하여 살펴보면 `generates random deviates.`라고 기술되어 있는 것을 확인할 수 있습니다. 이어지는 함수를 풀어서 설명하면 다음과 같습니다.: 다음의 결과를 정수의 형태로 저장하라(`as.integer`) $\rightarrow$ 무작위로 다음의 범주 내에서 다른 값을 1번 추출하라 $\rightarrow$ 최소값은 1, 최대값은 6 (1 이상 7미만)을 갖게하라.

그러면 이번에는 두 개의 주사위를 굴려보도록 하겠습니다. 굴리는 횟수는 한 번입니다.

```{r}
dice <- (as.integer(runif(1, min=1, max=7))) +
  (as.integer(runif(1, min=1, max=7)))
dice
```

여기서 `+` 연산자는 부울리안 논리에 따르면 `OR`를 의미합니다. 즉 각 주사위를 한 번씩 랜덤으로 굴려 얻는 값을 더한 결과를 `dice`에 저장하라는 명령입니다. 그럼 이번에는 100번, 1000번, 그리고 100만번을 돌려겠습니다.

```{r}
# 주사위 두 개를 100번 던져보기
dice100 <-  (as.integer(runif(100, min=1, max=7))) +
  (as.integer(runif(100, min=1, max=7)))

# 주사위 두 개를 1,000번 던져보기
dice1000 <-  (as.integer(runif(1000, min=1, max=7))) +
  (as.integer(runif(1000, min=1, max=7)))

# 주사위 두 개를 100만 번 던져보기
dice1M <-  (as.integer(runif(1000000, min=1, max=7))) + 
  (as.integer(runif(1000000, min=1, max=7)))
```

이렇게 시뮬레이팅한 세 결과를 히스토그램으로 살펴보겠습니다.

```{r, fig.height=2.5}
par(mfrow = c(1, 3))
hist(dice1000)
hist(dice100)
hist(dice1M)
```

일단 가시적으로 살펴볼 수 있게 각 코드 이후에 그 결과값의 빈도를 표로 나타내보았습니다. 그리고 그 표를 히스토그램으로 재구성해보았습니다. 역시 `N`이 늘어날 수록 우리(?)가 사랑하는 그 녀석의 모습이 드러나기 시작합니다.

주사위는 1에서 6까지의 한정된 값을 가지고, 두 개를 합쳐서 굴려봐야 2부터 12까지의 한정된(bounded) 값이긴 하지만 이 주사위 굴리기를 통해서 우리는 지난 번 포스팅에서 살펴보았던 것처럼 정규분포(normal distribution)와 표본 크기(n), 혹은 표집(sampling)의 관계를 간접적으로 다시 한 번 살펴볼 수 있습니다.


## 동전 던지기

그럼 이번에는 동전을 한 번 던져보자. 나는 아직까지 앞면 뒷면 이외에 옆면에도 표기를 지닌 동전을 본 적이 없으니, 여기서의 동전도 앞면과 뒷면이라는 두 개의 값만을 가진다고 가정하자. 삼면이나 사면을 가진 동전을 보신 분들은 부디 댓글로 알려주시길... 나와라 검은 백조야(김웅진 · 김지희 2012, p.53).
아무트 앞면과 뒷면이 있는 경우에 그 각각이 나올 확률은 0.5, 0.5라고 할 수 있다. rbinom은, randomly [drawn] binomial, 무작위로 이항변수를 추출하라는 함수라고 할 수 있다. 백문이 불여일코드.

```{r}
coin <- rbinom(1, 1, .5)
coin
```

이어지는 함수를 풀어서 설명하면 다음과 같다.: 이항변수로 무작위로 추출하라(rbinom) → 1번 추출하라 → 최대값은 1 (=최소값은 0) → 추출확률은 0.5. 즉, 1이 나올 확률을 50%로 설정하여 무작위로 추출하라는 것이다. 그럼 이번에는 100개의 동전을 던져보자.

```{r}
# 동전 100개 던지기
coin100 <- rbinom(100, 1, .5)
coin100 %>% table() %>% kable()

#동전 1000개 던지기
coin1000 <- rbinom(1000, 1, .5)
coin1000 %>% table() %>% kable()

par(mfrow = c(1, 2))
hist(coin100)
hist(coin1000)
```

이와 같이 이항변수는 나뉘어진(discrete) 값을 가진다. 히스토그램으로 그리면 0이 나오는 빈도랑 1이 나오는 빈도만 보여주는 것이다. 이번에는 100개의 동전을 1000번 던지는 경우를 시뮬레이팅해보자.

```{r}
coin1Mx <- rep(NA, 1000000)
for(i in 1:1000000){
coin1Mx[i] <- sum(rbinom(100, 1, .5))
}
hist(coin1Mx, 
     freq = FALSE, 
     main = "Distribution of heads\n in 100 coin tosses", 
     xlab = "Number of heads")
```

100만 개의 셀을 결측치(NA)로 갖는 텅빈 coinMx라는 벡터를 만들어보자. 그리고 i가 1에서 100만까지 반복되는 loop를 구성하자. coin1Mx_1부터 coin1Mx_1000000까지 총 100만개의 coin1Mx_n들은 모두 100개의 동전을 던져서 앞면(1)이 나오는 경우의 수를 더한 각각의 값을 가진다. 따라서 coin1Mx는 100만개의 요소를 지닌 벡터이다.

이 자료를 활용해서 만약 100개를 던졌을 때, 앞면이 60번 나오는 것이 과연 극단적인 확률일지 아니면 무던한 것일지 확인해보자. 60번 이상 앞면이 나온 경우를 세는 함수를 짜보자.

```{r}
table(coin1Mx[coin1Mx > 60])
coin1Mx[coin1Mx > 60] %>% table() %>% t() %>% kable()
```

앞면이 나오는 빈도가 61번 이상부터는 점차 감소하는 것을 확인할 수 있다. 그렇다면 이번에는 앞면이 60번 넘게 나올 확률을 구해보자.

```{r}
length(coin1Mx[coin1Mx > 60])/length(coin1Mx)
sum(table(coin1Mx[coin1Mx > 60]))/1000000
```

length는 count랑 같은 개념이라고 할 수 있다. 전체 coin1Mx의 수, 즉 100만 건 중에서 앞면이 60번보다 많이 나온 경우의 수가 어느 정도인지를 계산한 것이다. 그 결과 우리는 0.017561라는 값을 얻을 수 있다. 아래의 함수를 이용해서도 동일한 결과를 얻을 수 있다. 즉, 앞면이 60번보다 더 많이 나올 확률은 매우 작다고 할 수 있다. "평균적" or "일반적"인 것은 아니다.

## 독립사건 시뮬레이션

두 개의 독립적인 사건을 시뮬레이션해보자. 첫 번째 함수는 rand1이라는 자료에 최소값0, 10 미만의 값을 가지는 100개의 수를 무작위로 담으라는 명령이다. 두 번째 함수는 정규분포를 따라 평균이 0이고 표준편차가 2의 값을 갖는 분포에서 100개의 값을 무작위로 추출해 rand2라는 자료에 담으라는 명령이다.

  + 일단 보면 rand2의 경우, 비록 명령으로 평균을 0으로 하라고 설정했지만 무작위 추출 결과 100개의 값의 평균은 0보다 약간 큰 것을 확인할 수 있다.
  + 이제 두 값을 각각 x축, y축으로 설정하여 100개의 값을 좌표에 매칭시킨 그래프로 나타내자.


```{r}
rand1 <- runif(100, min = 0, max = 10)
summary(rand1)
rand2 <- rnorm(100, mean = 0, sd = 2)
summary(rand2)
plot(rand1, rand2)
```

결과적으로 이 그래프에서 우리는 rand1과 rand2 간에는 어떠한 경향성을 발견하기 힘들다. 즉, 독립적이라는 것은 두 변수 간에 어떠한 관계를 상정할 수 없다는 것으로 이해할 수 있다.

## 종속사건 시뮬레이션

이번에는 두 사건이 종속적 관계에 있는, 즉 한 사건에서의 변화가 다른 사건에 영향을 미치는 경우를 시뮬레이션해 보자.

```{r}
rand3 <- 4 + 0.75 * rand1 + rnorm(100, mean = 0, sd = 2)
```

rand3는 아까 만들어놓은 rand1에다가 0.75를 곱해서 4를 더한 100개의 값에다가 사실 상 rand2와 같은 방식으로 구한 값을 더하여 구한 자료이다. 즉, 이 값은 rand1의 값에 어떠한 조치를 취하여 얻은 값이므로 rand1에 영향을 받은 결과물이라고 할 수 있다. 이렇게 구한 rand3와 rand1 간의 관계를 그래프로 나타내보자.

```{r}
plot(rand1, rand3)
```
이 그래프의 해석은 나중에 simple regression 및 scatter plot을 살펴볼 때 다시 한 번 다룰 것이지만, rand1이 증가할 때, rand3도 증가세를 보이는, 둘의 인과적 관계는 확인할 수 없지만 아무튼 양(positive)의 관계를 보이고 있다. cor(rand1, rand3)로 두 변수 간의 상관계수를 구해보면 0.640155가 나온다. 뒤의 rnorm을 이용하여 생성한 100개의 값을 구해서 이 결과는 구할 때마다 달라질 것이다. 마찬가지로 무작위로 추출한 100개의 값을 더한 결과이므로 초기 식의 0.75라는 기울기와는 다르지만 여하튼 양수의 기울기를 얻게된 것이다.

이번에는 보다 더 불확실성을 가지는 종속사건의 관계를 시뮬레이션해보자. 여기서 말하는 불확실성을 반영하기 위해서 더 큰 표준편차를 의미한다. 표준편차가 평균에서 각 관측치가 떨어져 있는 거리의 평균이라고 할 때, 이 값이 크다는 것은 개별 값들이 평균에서 더 넓게 분포해 있다는 것을 의미한다.

```{r}
rand4 <- 4 + 0.75 * rand1 + rnorm(100, mean = 0, sd = 5)
plot(rand1, rand4)
cor(rand1, rand4)
```


rand1과 rand4의 산포도는 좀 더 넓게 퍼진 모양새를 보이고 둘의 상관관계는 식에서 상정한 0.75라는 기울기에서 0.456으로 더 낮아지는 것을 확인할 수 있다.

늘은 지난 번에 이어서 확률을 보고 거기다 더해서 분포를 살펴본다. 즉 오늘의 주제는 "R에서 흔하게 사용하는 분포 함수(distribution functions) 살펴보기"라고 할 수 있겠다.
· 정규분포(normal distributions)와 이항분포(binomial distributions)로부터 분위(quantiles)를 얻어내는 방법
· R에서 흔하게 사용하는 분포 함수(distribution functions) 살펴보기
마지막 두 개는 오늘 포스팅에서는 사용하지 않지만 평소에 내가 많이 사용하는 패키지들이다. 앞에 #을 붙이면 R 콘솔은 이것을 코멘트로 인식해서 실질적인 R코드로는 작동하지 않는다.
그럼 먼저, 지난 번 포스팅에서 하려다가 못하고 마무리 지은 정규분포(normal distributions)와 이항분포(binomial distributions)로부터 분위(quantiles)를 얻어내는 방법을 살펴보자.

## 정규분포 (The normal distribution)

```{r}

pnorm(70, mean = 50, sd = 10, lower.tail = TRUE)

pnorm(70, mean = 50, sd = 10, lower.tail = FALSE)

1 - pnorm(70, mean = 50, sd = 10, lower.tail = TRUE)
```

즉, 첫번째 식은 50을 평균으로 하고 10을 표준편차로 하는 정규분포가 있을 때, 그 분포에서 70이라는 숫자는 어디에 위치하는지를 묻는 것이다. 두 번째도 동일한 의미인데, 두 식의 차이는 lower.tail 옵션에 달려있다. 각 식이 도출한 결과를 보면 이해하겠지만 근본적으로 두 식은 동일하며, 좌측에서 누적확률을 계산할 것인지 우측으로부터 계산할 것인지의 차이일 뿐이다. 간단하게 말하면 첫 번째 식은 70이라는 숫자는 이 분포에서 하위 97.7%에 위치한 값이다라고 말하는 것이고 두 번째 식은 상위 2.2%라고 말하는 것이다. 따라서 두 번째 식은 전체 확률에서 첫 번째 식으로 계산한 확률을 제한 값과 같으므로 세 번째의 형태로 계산할 수도 있다.
그렇다면 만약에 양측꼬리 확률(two-tailed probability)에서 최소한 70만큼 '극단적인'(extreme) 확률을 얻고 싶을 때는 어떻게 해야할까? 이 경우는 생각을 좀 달리 해봐야 한다. 평균을 기점으로 70은 오른쪽 끝쪽에 위치한다. 그만큼 왼쪽 끝에 위치한 값을 상정하고 그 값이 나올 확률을 함게 계산해주어야 한다는 것이다. 이 경우 평균 50에서 70은 20만큼 떨어져 있다(우측으로, + 방향). 따라서 우리는 좌측으로 20만큼 떨어진 30이 나올 확률을 함께 고려해주어야 한다. 이렇게 하는 데에는 두 가지 방식이 있다. 첫 번째 식이 좀 더 직관적으로 이해할 수 있을 것이며, 두 번째 식은 보다 손쉽게 하는 방법이다.

```{r}

pnorm(70, mean = 50, sd = 10, lower.tail = FALSE) + 
  pnorm(30, mean = 50, sd = 10, lower.tail = TRUE)

2 * pnorm(70, mean = 50, sd = 10, lower.tail = FALSE)

```

그렇다면 이렇게 구한 분포에서의 누적확률 값을 가지고 분위를 구하여 보자. 마찬가지로 평균 50에 표준편차가 10인 분포를 상정한다. 이때 사용할 함수는 qnorm이다.

```{r}
qnorm(0.9772499, mean = 50, sd = 10, lower.tail = TRUE)
```

역으로 계산한 것인데, 평균 50에 표준편차 10인 분포에서 좌측부터 누적확률로 0.9772499에 해당하는 값을 구하라는 명령이다. 앞에서 우리가 입력한 70과 근사한 값을 얻을 수 있다. 근소한 차이는 소수점에 의해 발생하는 것으로 이해할 수 있다.
다음으로는 주어진 평균 50, 표준편차 10의 분포에서 70이라는 값이 분포에서 차지하는 밀도를 확인해보자 밀도(density)를 알아보기 위한 함수는 다음과 같다.

```{r}
dnorm(70, mean = 50, sd = 10)
```

그렇다면 이번에는 정규분포에서 무작위로 추출(draws)을 해보자. 이번에도 함수에는 70이라는 값이 사용될 것인데, 여기서 사용되는 70은 특정한 값이 아니라 추출횟수를 의미한다.

```{r}
x <- rnorm(70, mean = 50, sd = 10)
x
```

총 70개의 값이 무작위로 추출되어 x라는 벡터에 담겨 있는 것을 확인할 수 있다.

## 이항분포 (Binomial distribution)

이항분포는 우리에게 n번의 시행에서 k번 성공할 확률을 보여주는 분포이다. 단순하게 말하자면 0과 1의 값만 갖는다고 가정된 벡터가 있다고 하자. 이때 벡터의 요소의 총 개수는 100개이고 랜덤으로 0과 1 중 하나가 벡터에 들어간다고 하자. 이때 전체 요소의 수 100개 중 1이 뽑혔을 경우를 계산하면 결국 1을 뽑을 성공 사례의 총합을 100으로 나눔으로써, 전체 대비 성공의 확률을 구하는 것이다. 즉, 정규분포에서와는 다르게 이항분포에서는 평균(mean)이 아니라 비율(proportion)에 초점을 맞추게 된다.

```{r}
pbinom(27, size=100, prob=0.25, lower.tail = TRUE)
```

시도가 성공할 확률을 0.25라고 놓고 시행 횟수를 100번으로 가정한 경우이다. 즉, 100번 시도했을 때 성공할 확률이 25%인 이항분포를 가지는 데, 실제로는 27번 성공했다고 한다면 누적확률에서 어느 위치에 해당하는지를 묻는 함수이다. lower.tail이 TRUE로 설정되어 있으므로 좌측에서부터 계산한 것이다. 즉, 27번 성공한 것은 하위 72.2%, 상위 27.8%에 해당한다는 것을 알 수 있다.

```{r}
qbinom(0.7223805, size = 100, prob = 0.25, lower.tail = TRUE)
```

이렇게 구한 누적확률로 다시금 성공 횟수를 추정해보자. 정규분포때와 거의 유사하다. size = 100은 서로 독립적인 사건의 시행, 즉 베르누이 시행(Bernoulli trials)의 횟수를 의미한다. 그렇다면 100번 시도했을 때 27번 성공할 정확한 확률을 구해보자.

```{r}
dbinom(27, size=100, prob=0.25)
choose(100, 27)*.25^27*(1-.25)^(100-27)
```

두 가지 방법으로 구할 수 있는데, 첫 번째는 R에 내장된 기본 함수인 dbinom으로 구하는 것이다. 두 번째는 choose 함수를 이용해서 구하는 것이다. 개인적으로는 dbinom 함수가 있는데 굳이 choose 함수 사용법까지 알아야 하나 싶기는 하지만, choose 함수로 보여지는 식이 좀 더 직관적으로 이해하기에는 도움이 된다.

```{r}
rbinom(27, size=100, prob=0.25)
sd(rbinom(27, size=100, prob=0.25))
```

마지막으로는 0.25의 확률을 가진 100번의 베르누이 시행을 27번을 반복하는 결과를 보여준다. 즉, 평균적으로는 100번 시행 중 25번의 성공을 할 것으로 기대되지만, 실제 시행을 27번 해보면 무작위 추출이기 때문에 그 25를 중심으로 표준편차 4.21의 범위 내에서 여러 값들이 추출되는 것을 확인할 수 있다. 무작위기 때문에 돌릴 때마다 값은 다르게 나온다.

## R에서의 분포 (Distribution in R)

### 졍규분포 (Normal distribution)

이번에는 좀 더 구체적으로 정규분포 사례들을 살펴보자. 서로 다른 평균(mean)과 표준편차(standard deviation)을 가지는 세 개의 정규분포를 그려볼 것이다.


```{r}
normal5 <- rnorm(n = 10000, mean = 5, sd = 3)
normal50 <- rnorm(n = 10000, mean = 50, sd = 10)
normal20 <- rnorm(n = 10000, mean = 20, sd = 1)
```

세 함수 모두 표본의 크기는 1만개이며, 각자 다른 평균과 표준편차를 따르는 정규분포를 가정하여 무작위로 추출된 값을 담는 벡터로 산출된다. 이렇게 값을 갖는 세 개의 벡터를 하나의 데이터프레임으로 합쳐보자. 간단하게 말하면 하나의 표로 합친다는 것이다.

```{r}
norm <- as_tibble(rbind(cbind(x = normal5, Mean = 5, SD = 3),
                         cbind(x = normal50, Mean = 50, SD = 10),
                         cbind(x = normal20, Mean = 20, SD = 1)))
norm$Mean <- as.factor(norm$Mean)
```

그리고 이렇게 만들어진 데이터프레임의 평균 값을 Factor 자료 유형으로 변환해준다. 이것이 의미하는 게 뭘까? 평균5, 평균50, 평균20을 문자열로 인식하게 하여 일종의 "이름"으로 인식하게 만드는 것이다. 그러면 이제 이렇게 만들어진 데이터프레임으로 그래프를 그려보자.

```{r}
ggplot(norm, aes(x = x)) +
  geom_density(aes(fill = as.factor(Mean)), adjust = 4, alpha = 1/2) +
  guides(color=guide_legend(title = "Mean, SD")) +
  guides(fill=guide_legend(title = "Mean, SD")) +
  scale_color_discrete(labels = c("5, 3", "20, 1", "50, 10")) +
  scale_fill_discrete(labels = c("5, 3", "20, 1", "50, 10")) +
  ggtitle("Probability Density Function\nNormal Distribution")
```

ggplot2 패키지는 R에서 그래프를 그리는 데 있어서 유용하게 사용된다. 먼저 ggplot2 패키지를 불러오고 나서, ggplot(데이터프레임 이름, aes(x = x축으로 삼을 변수이름))을 설정하면 일단 수학적으로 지정한 데이터프레임의 변수에 대한 기본적인 작업은 진행이 된다. 그러나 이 단계에서는 아직 가시화(visualization)라고 할 수는 없는데, 주어진 데이터를 컴퓨팅했을 뿐이지 어떻게 가시적인 형태로 보여주라는 명령을 부여하지 않았기 때문이다. 컴퓨터로 그림을 그려본 사람들은 이해가 쉬운데, ggplot은 레이어 시스템을 이용해서, 우리가 뭔가 그려진 걸 얻고 싶을 때마다 레이어를 하나씩 추가해서 보여달라고 R에게 요구해야만 한다.

그리고 이때, 레이어를 추가하는 것은 '+'로 가능하다. 하나씩 뜯어서 보면,

  + ggplot(norm, aes(x = x)) + : ggplot 패키지를 이용하여 norm이라는 데이터 프레임에서 x축에는 x라는 변수를 기준으로 늘어놓아라. 그리고 뒤에 더해지는 레이어 명령을 덮어 씌워라.
  + geom_density(aes(fill = as.factor(Mean)), adjust = 4, alpha = 1/2) +: 밀도함수의 형태로 그래프를 그리되, Mean이라는 Factor 변수가 같은 것들 끼리 같은 색으로 칠해서 보여주어라. [뒤의 옵션은 세세한 조정이니 굳이 언급하지는 않겠다.] 그리고 뒤에 더해지는 레이어 명령을 덮어 씌워라.
  + guides(color=guide_legend(title = "Mean, SD")) + guides(fill=guide_legend(title = "Mean, SD")) +: 우측에 더해지는 레전드의 이름을 어떻게 지으라는 명령. 이 경우에는 색도 있고 그래프를 선명하게 보여주는 선도 있기 때문에 그 각각이 레이어를 이루고 있어 둘 모두에게 이러한 명령어를 적용해야 한다.
  + scale_color_discrete(labels = c("5, 3", "20, 1", "50, 10")) +  scale_fill_discrete(labels = c("5, 3", "20, 1", "50, 10")) +: color와 fill을 뒤에 이어지를 라벨로 구분해주라는 명령어이다.
  + ggtitle("Probability Density Function\nNormal Distribution") : 표 전체의 이름을 지정하는 명령어이고, \n은 R에서는 강제 개행(enter)하는 명령어이다.

### 이항 분포 (Binomial distribution)

n번의 독립적인 베르누이 사건을 시행할 때, k번 성공할 확률을 구하는 분포이다. 여기서는 구체적으로 모집단의 성공 확률을 p, 모집단의 크기를 n으로 특정한다.

```{r}
binom10 <- rbinom(n = 10000, p = .5, size = 10)
binom50 <- rbinom(n = 10000, p = .5, size = 50)
binom100 <- rbinom(n = 10000, p = .5, size = 100)
```

정규분포와 다르지는 않다. 다만 여기서의 size는 전체 동전을 던지는 횟수로 이해하면 되고 10000은 그렇게 동전을 10번, 50번, 100번 던지는 걸 10000번 반복한다는 의미라고 할 수 있다. 그럼 이제 마찬가지로 하나의 데이터프레임으로 세 번의 시도(binom10, binom50, binom100)의 결과를 합쳐주고 그래프를 그려보자.

```{r}
binom <- data.frame(rbind(cbind(k = binom10, Size = 10), 
                          cbind(k = binom50, Size = 50), 
                          cbind(k = binom100, Size = 100)))
binom$Size <- as.factor(binom$Size)

ggplot(binom, aes(x = k)) +
  geom_density(aes(group = Size, color = Size, fill = Size), 
               adjust = 4, alpha = 1/2) +
  ggtitle("Probability Mass Function\nBinomial Distribution")
```

### 포와송 분포 (Poison distribution)

이 포스팅은 주로 R코드에 관한 것이기 때문에 분포에 대한 수리통계적인 설명은 가급적 피하도록 하겠다. 포와송 분포는 고정된 대규모 모집단(fixed large population)에서 독립적인 개체들이 짧은 시간에 걸쳐서 희소한 사건(rare events)의 발생 횟수를 추정하는 데 용이한 분포이다. 포와송 분포에서 그 희소한 사건의 발생 확률은 시간 단위 별 발생의 평균 횟수로 나타나며 그리스어 람다(Lambda)로 표기된다. 람다는 평균과 분산을 결정한다. 포와송 분포의 확률을 이용해서 우리는 단일 시간 단위에서 k라는 희소한 사건을 정확하게 관측할 확률을 기술할 수 있다. 나머지는 앞의 정규분포랑 이항분포에서 했던 코드를 기계적으로 반복해서 살펴보자. 단, 포와송 분포에서 특히 고려해줘야 하는 부분의 코드는 이전과 다르니 그 점만 유의하면 될 것 같다.

```{r}
pois1 <- rpois(n = 10000, lambda = 1)
pois2 <- rpois(n = 10000, lambda = 2)
pois5 <- rpois(n = 10000, lambda = 5)
pois20 <- rpois(n = 10000, lambda = 20)
pois <- as_tibble(Lambda.1 = pois1, 
                  Lamnda.2 = pois2, 
                  Lambda.5 = pois5, 
                  Lambda.20 = pois20)
```

이번에는 만들어진 pois라는 데이터프레임을 melt 함수를 이용해서 다른 변수로 재구성해줄 것이다. melt 함수는 reshape2 패키지에서 로드할 수 있고, str_extract 함수는 stringr 패키지에서 로드할 수 있다.





<!--chapter:end:03-Prop2R.Rmd-->

